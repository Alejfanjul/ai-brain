# Memory Lane - how I automatically turn Claude Code sessions into a reusable learning + memory system

## Fonte
- **Tipo:** video
- **Autor:** Alex Hillman
- **URL:** https://www.youtube.com/watch?v=Wpz7LNI737Q
- **Duração:** 34 min
- **Data original:** 2025-12-21
- **Data captura:** 2026-01-04

## Conteúdo

Kind: captions Language: en you've been following along with my claude code powered executive assistant and the JFDI system that I've kind of built around all of it. You know, I went from showing sort of the big picture got a lot of folks really excited, a lot of questions. That's been fun. The last few days, it's shifted more into like, well, how did you do that one thing? And a couple days ago, I showed the way I had been doing memory with a focus on learning with an audit trail system. And that part is not going away. But I wanted something that was a little more on the fly like the learning system is looking at the actions and like once a week looking for pattern and improving SOPs and workflows and building new features. That is not going to change in the system. That has been so powerful. But I I want to like basically made the agent that I interact with every day my main assistant whose name is Andy. There's a little bit of an inside joke from Andy Hall. But I wanted Andy to have memory. You know, I think that the the document-based approach was working really well, but I found myself still very often having to provide a lot of information up front over and over and over again, especially around repeat stuff that wasn't quite a, you know, command or a skill. But in conversation, I was like, you you know this, we've talked about this before, but obviously it doesn't know anything. It needs to recall those things. And so started doing some research on how people are approaching the idea of memory around agents and models and then started prototyping this thing that I called memory lane as a way to generate memories and then had a real breakthrough last night. Actually, somebody who comes to Indie Hall, the co-working community that I run, was there for Philly RB, our local Ruby meetup, and we were talking about this stuff and a couple days later, he sent me a link to I think Google published a paper, a research paper about kind of memory that they are building actually into the model, into the LLM itself. And one of the key insights was that this idea of surprise and if you think about it, you know, a lot of this is biomimicry. We are trying to understand how things work in nature and going how do we make technology kind of mimic what happens in nature. And so the biomimicry of memory is thinking about things like long-term and short-term. What triggers cause a memory to form? What triggers cause a memory to be recalled? What triggers cause us to lose memory? and kind of being inspired by those ideas, not necessarily copying them and going, "Well, if those things work in in in our actual human brains, what is a version that could maybe empower an agent who has no context between any individual message that gets sent back and forth? The model has no idea. It's just processing the back and forth, right?" And so, I came up with a few ideas and then that paper really kind of pushed me over the finish line. And I want to show you kind of what I've built over the last four or five days or so cuz it's pretty cool. And this is going to get a little more technical. I'm not just going to be showing off the features. I'm going to start there because you see some of this on my screen. I want to show you how this works. But then I've got like an architecture document that we can go through together as well. So the way that this system really began and we're looking at the memories page here, but actually makes more sense to go into my quad code sessions because this is where memory starts. And if you haven't actually dug under the hood of cloud code, you might not realize that all the back and forth that you do, all the stuff that appears in the terminal is also being saved to local JSON file. I think it's a JSON L file. I don't even know what JSON L versus B versus normal good old fashioned JSON is, but it is all the back and forth with the agent. It's all your tool calls. It's all of the files that it touches and all this other useful information. And it's actually pretty smart about how it does it. It doesn't save things like like the responses that might include things like secrets and so forth. So, it does in a pretty smart way and it's all kept on your local machine. If you've noticed over time of using claud code that it gets kind of slow, it's because that pile of JSON files is piling up and it has to read all of them. That's how it, you know, if you can resume a session or rewind, that's all just basically reading those files. And so, I've been using this long enough that my startup was getting kind of slow. And so I wanted to get that stuff off disk, but I didn't want to throw it away. And so initially I was just doing a basic backup and then I was like, no, I want to make it useful instead of just sticking it in a cold storage somewhere. And so I I set up a table in the JFDI system in the database, which by the way using Superbase, which has been awesome. So there's a table in Superbase for my claude chat sessions and every session gets its own row, right? And so that happens. I want to say it syncs like every 5 minutes or so. It's sending those JSON files up and it's stuffing the entire transcript, the entire JSON file, which you know, it's just text, but in some of my really long chats can be 7, 10, maybe 15 megabytes if I'm really going kind of bananas and and running very long sessions with a lot of compacting and things like that. I try not to do that very often, but but I also do a little bit of processing as I'm doing that where, you know, one column is full transcript, the next one is going to be the first message, the next column is going to be the most recent message. Also, timestamps for both of those. I pull out separately all of the messages that I say. So, this is my input to the chat session. I put in all of the messages that the agent generates. So, we can keep those things easy and separate and recallable on their own. Also in the Amazon column, things like tool calls, what kind of work it was, what files were touched, and so on and so forth. And that all gets automatically generated. That's been I've been doing that for quite a a few weeks now. And it was sort of like a backup plus+, but it also gave me a my own interface for being able to recall old messages without the slowness of the startup of Cloud Code, which I'm barely going into anyway. And so like you know in this little drop down center here like this is basically pulling from the same thing. And you can see some things are mirrored here like I've got pin sessions. This little sparkle actually lets you I should say that's me. Let me pick one that's not going to take forever. So basically that little button takes the chat transcript, throws it against the haiku model and has it do autogenerate a nicer title rather than just whatever my first message happened to be, which sometimes makes it easier to find these things. It's cheap. It's relatively fast. It's something that I might have it just start doing automatically, but you can see it turned that from whatever it was to debug memory lane, not what that was. So this is in a way its own kind of memory. But the way I think about it, it would be the equivalent of if the way you remembered things is you remembered everything you said, everything the other person said, everything you both did. That's not very useful memory. It's just it's it's almost too high resolution. There's too much information there. And so recall from that would take a whole lot of work. So what I started prototyping was a way to also basically once a memory is in the database how could I start I'm sorry once the sessions in the database how could it extract memories from the data base what would the qualification for that be and so I basically have another job that runs roughly every 15 minutes and it looks at all new information in this table. So it does do incremental updates and all those kinds of things. And I call it a memory catcher. And so what the memory catcher does is it is going through those chats and it is looking for what I would call moments of consequential decision which another indie call member pointed out is like a really freaking awesome band name. But moments of consequential decision includes a handful of things. So we have decisions outright. We have insights, patterns, commitments, moments of learning, correction in either direction. Like usually me correcting the agent, but sometimes it's the agent correcting me. I make an assumption. It's like, that's not how that works. Workflows and then gaps. Gaps are really cool is when it notices two parts of the system that are related but not talking to each other. And if there was a talking to each other that there would be an improvement in performance output, whatever it might be. So what happens is it's looking at the entire transcript looking for those moments and and extracting the information out into smaller little chunks. And so I know folks if you're familiar with like rag retrieval augmented generation chunking is like one of the key parts in like getting the components that are retrievable to the right size and context and things like that. And so this is all being done with the model. It's being it it's using that to decide what to pull out, how to structure it, following all of those rules. And then it generates memories with all of these pieces of information. And so, you know, there's a title, what kind of thing. This is technical learning. You come into another one. This is systems learning. This is a relationship insight. What else do we have in here? We've got workflows. As you can see, there's a couple little things that might duplicate in here. Here's an interesting example of a correction. at the systems level. But so let's look while we're in here like what's going on. So it's keeping track of when the memory was formed. That is the time stamp of when either I said the thing or the agent said the thing that is when the memory was formed and then when the memory was saved is usually like I said about 15 to 20 minutes later is when it this database this table and everything except for original context is being generated on the fly by this process. So it creates this brief little summary. The reasoning why it chose it is a huge piece to this little this little part of the system. We'll come back to that in a second. Confidence score is helpful for filtering out stuff below a certain threshold. Related entities can be anything from in my JFDI system I have people and projects and reminders and things like that. And so those are all entities. Events, documents, those are all entities. But then also files within the entire basically any file on the file system can be an entity as well. And that's useful for basically recalling memories associated like if I touch a file or if the agent touches a file it should go look up memories about last time that file was touched. That's potentially super handy. it saves the chunk of the original context and so it is going to look you know some threshold above and below and it's going to format it in this consistent markdown way that's easy to read and it's going to provide the link to the source session as well and so all day long as I'm working as I'm doing things as the agents doing things because remember I've got a bunch of jobs that run on various schedules to and a lot of that is sort of agents being invoked or workflows slashcomands skills being invoked automatically to either do prep work or all the admin stuff that makes it so when I sit down to work things are ready for me work also gets read and turned into memory. So this isn't just when I'm actively talking to it. It's when claw code does anything. It is generating all these memories. And there's a few other things that are that are kind of neat in here. Like I said, this is filterable into these categories. It's sortable. I'm not going to go too much into this entity queue in part because I'm still I end up figuring out exactly where like how to close some gaps here, but basically we'll come back we'll come back to that in a second. And then while I'm here, I I'll show you that there's this fun little visualization you might have seen me share a couple nights ago as well where I said, "Hey, this is kind of fun." And I was inspired by my my business partner Amy Hoy and her partner Thomas many years ago made this really cool visualization called twist story that was just reading from Twitter data at the time and making a cool kind of inspiring and and this is not nearly as cool or inspiring but that was kind of where my head went. I said let's see what this thing can do and so it does this like river view of all the memories as they're coming through which is kind of a fun visualization. And then I also had it build this heat map that is it's kind of neat. And maybe you can see on the video is twinkling a little bit, but each of these I can mouse over it. And you I want to play with things where like if I click on this, they kind of like all kind of shift around. It pulls the one into view and shows all the things that are closest connections to it. This is more of a toy than the utility. But cool, right? I want to show you the like the cool and useful part cuz it's over here on the right. Now, you'll see that my my chat window has something new. I'm going to open this up all the way so that we don't get distracted by the animations over there. So, so we're storing all of those memories. That's half of the equation. And I would argue the easier half of the equation. It was much easier than I thought it was going to be. It really comes down to like one main prompt. We'll get to that in a second. The retrieval part is where things are powerful and challenging. I'm finding and part of that is I had to like learn a bunch of stuff like the fundamentals of rag. I had to really wrap my head around what embeddings are, how they work, and how semantic retrieval works compared to just the textbased search which was I was getting very good results but everybody kept saying like you got to try vector search and I was like all right fine. And so I got a great bit of code from my buddy Joel Hooks who has this GitHub project called semantic memory that he was using. in the demo that he gave me was it was a lot of like I'm doing project planning and so here is a body of books that I reference while you're planning reference these books for best practices because these are the experts that I listen from anyway and that's a really cool idea and I was like that's not as useful for me in my day-to-day for the kind of work that I do but I would like to use I like that the system use its own body of knowledge to kind of recurse on itself and make it so that I don't have to remind it all the time of certain things. So that's what I did. So I kind of combined Joel's idea with a few of my own, some other best practices, and then that surprise idea from Google. Remember, Google's idea is at the model level where it's using surprise to kind of like weight things that will be surfaced. I'm doing it at the retrieval side. And so there's basically two pieces to this. Actually, we'll get we'll get into how the hooks work, but I'm using claude code hooks to at certain points when I type a message and hit enter or when the agent sends a message or invokes a tool call or something like that, there's a little bit of logic going on and decides when to do it, when there's enough information, when there's not enough information for it to be useful. And then it it's a little script, basically a shell script that does a little bit of searching for me. And the searches kind of layer on top of each other. It's doing a combination of text search and semantic search. And what's cool is is in real time basically as I'm working I get a little bar that pops up. I'm going to see if I can do it. Part of the challenge of this is like it you in the same way you're not in control of when the memories pop up. I'm not entirely in control of when the memories pop up. I know what things generally get it to do with. So, I'm going to mention like person and say like, you know, uh uh when is my and Adam's next meeting? And I'm going to very intentionally leave Adam's last name out because in theory what it should do is that part of that tool will look and notice a name or a thing that it thinks is a name or again an entity and it'll say I think that's a name and there's more than one atom in the system. Boom. Which atom did you mean? So it's going to pull out a few different atoms. In this case I meant Adam Tetterus. And that is sort of a a clarifying step that happens automatically anytime I mention an entity again, a person, a project, a reminder, an event, and it's like, hey, there's a few by the same name. Which one do you actually mean? So, I'm going to do that. And it will use that information to then clarify the rest of the question and the rest of the actions that it takes. And in this case, it's going to look at events. That's not actually the best thing for it to be looking at, but nonetheless, I'm going to stop that. These little bubbles up here are really what I want to show off because this is where the magic really happens. So, when it invokes a memory, it is either going to do again an entity memory, which is these little yellow ones, or a semantic memory, which is memories that it thinks are related for some based on meaning, not based on a term necessarily. So these are pretty straightforward where you know if I were to mention a specific person it will go find information about that person. What I actually one of the really key things here is anytime it touches a relationship file it's going to invoke memories related to that person. Not every memory is in the relationship file. And so this notices not just what the person and I talk about but how that person shows up in all of this universe and is able to pull pieces of that into the chat here. The semantic ones where it gets really cool though where I get all of the memories that it's pulling here and this is a this is going to be weighted towards technical stuff because I've been building this you largely for the last couple of days and so it's a little bit self-referential. A lot of the memories that it has in the system are about the building of the memory system which has been useful while building the memory system but these were all pulled in real time. So I was working on something related to a feature and the system was noticed that it did a a vector search. It found memories that it matched with what we were talking about and then once they passed a certain set of thresholds loaded not just that that memory exists but it grabs the entire text. Remember when we came into memories we're storing like the actual B body and so like all of this if it's useful if it passes all threshold actually gets stuck into the context. So this is a way to build relevant context on the fly using all of these past memories which gives you in effect the ability to remember something. So I'm the agent's about to edit this file. Last time we edited this file I had to correct it because it made a mistake. It's going to load that correction in and most likely not make the same mistake again, right? Because I'm providing that as context. This is a way to build context on the fly. Way fancier than than I was doing before with just grapping across or searching across text files. So, what's happening in here? So, it shows me what kind of memory it is. Again, decision, a correction, learning. I get a a little bit of a tip here. here. And this is the thing I'm working on is like I I wanted to know what message caused it to pull in this memory so I can start getting a better sense of what kinds of messages invoke or recall what kinds of memories. Cuz the hardest part of all this is the vector search feels a little bit like magic. And and the best explanation that I've seen about how vector search works is it's basing things on like relative proximity of it guesses the meaning of words and phrases and sentences and more based on the words and phrases and sentences that are closest to them in this like three-dimensional plane mathematical equation way that like I'm going to be honest I still don't fully understand. But I'm getting a sense of how it works in the same way that I have a sense of how the LLM works. I don't fully understand all of the architecture. I know more than I did 6 months ago. There's a lot that I don't get, but I have a sense of how it gets that I think you can really only understand by tinkering with it at this level and then observing it. So, you know, it's doing that. It tells me when the memory was was actually from. So, this is from yesterday. And in this case, this memory was recalled 30 minutes ago. So while I'm in chat I can be like what memories are influencing that and when it should get great get recall 64% these these ratings these are how semantically similar the system thinks it is as one of the factors that goes into whether or not a memory is injected or included or recalled into a session or dropped or maybe it's kept but waited much less. Those are that's all built in here as well. That's kind of the algorithm I'm still playing with. And the last piece is your good oldfashioned thumbs up, thumbs down. If if I'm like, oh, the link between these two things makes sense, I give it a thumbs up. If it makes less sense, like this one, that's probably a good one. I give it a thumbs down. as that not only visually indicate here, indicates it here, but there is a table that keeps track of all of those and makes it keeps track of them in context again because like it's not that that matching is bad, it's that it wasn't is or isn't useful in whatever we're doing here. And so the next time the system does this loop, it's going to factor in those thumbs up, thumbs down as a plus or minus weight. I think it's like a plus or minus 5% per time I have given it a thumbs up or a thumbs down respectively. So over time this will get better and a lot of my that my thinking about this process is watching how quickly the the very unsophisticated audit trail got powerful and kind of compounding and I'm very quickly starting to see little bits and pieces here. still a little bit goofy and I think it's going to be better once I'm using it for not just while I'm building the system but actually having it track memories in my day-to-day in like how I do work rather than how it works technically speaking but functionally they should be about the same thing and what I might end up doing is different kinds of work like technical work on the system may have a different kind of waiting so to speak whereas my day-to-day work whether it's creative work or management work or communications work or community work or planning work. Well, those may have different weights and I may have another layer that like figures out what kind of work we're doing in a session and then adjusts that for memory lane, which again is sort of the name of this whole feature. So, you know, most of the time these are up here just kind of glowing and reminding me or or showing me that it is actually recalling these memories. And then I, you know, either something is going really right or going really wrong, I might peek at it. I don't know how much longer this stays a persistent bar, but for now it has been useful for me for learning. I think of this as a transparency tool. It's a way for me to understand how a tool that I built actually works compared to how I thought it works. But I do think that there's some long-term utility in the same way that, you know, having a collaborator, a human collaborator who is good at voicing why they're doing what they're doing is easier to collaborate. I kind of want the same thing from here, but it might end up being a toggle that I turn on and off for different kinds of work. We'll find out. But that's that's the general gist of memory lane and how it works in terms of the UI. Let's get into the architecture of it. So I I covered a lot of this already, but for folks that are more into sort of architectural diagrams and things like that, this actually, you know, I'm going to I'm going to share this document rather than talk about it. I'm going to give you sort of a high level of what's in here. So this is sort of that that workflow diagram of the overview how it's extracting learnings from sessions that are completed and also it says completed here but also it does incremental updates. So if I go back to a session or I pick up a session later those things will get picked up. The metadata and vector embeddings get stored all automatically. The vector embedding stuff by the way is all being done on device like that doesn't hit anybody's APIs. So it's another opportunity to keep all this stuff kind of kind of local using Olama and one of the popular I think it was the one that I borrowed from Joel. So whatever model he was using for embedding I was like cool I'll just use that. Postgres and PG vector for actually storing and retrieving those. The context hooks are the ones that are there actually quads hooks that run these little bits of code that decide when to search, when not to search, how to search, and then the waiting and all of those kinds of things. And then it gets injected into the new session. So the memory extraction, this kind of goes over the main types of memories and we also prioritize them. So you can and actually there's something in here that I want to change like this gap value. I would like it to be finding more of those and executing on more of those. That's maybe it's more useful for for the audit trail type workflow, but I'm maybe what I do is I when it finds them I have it promote them into you know feature ideas or something along those lines. Extraction triggers. This was the thing that I was inspired by Google much so much. So the system is looking explicitly for these four things. is looking for recovery patterns, meaning either of us tried a thing, it didn't work. We had to do it a different way and got it to work. That's a trigger of a memory, right? And that's the same way when you solve a problem, it's very satisfying because it was fail, fail, fail, succeed, right? And so you watch an agent work and it fail, fail, fail, succeeds. And that's very cool that it can succeed, but it's very frustrating when it doesn't remember that for next time. Now mine does. User correction. So basically, when I say, "Hey, you did it this way. I want you to do it this other way. It will detect that and it'll not only create that memory, it'll prioritize it. Enthusiasm signals. So, hey, you did like that's exactly what I wanted or wow, that's really cool. Which like it does feel weird to sometimes say that to a robot, but I find it does help that helps it in some way, but more importantly in this case, it's a signal that however it did it, I liked that. I want more of that in the future, which will again make that memory useful. Same thing in the other direction. actually negative user reactions also get caught. So it's like hey definitely never do that will which go in there and and be weighted appropriately and then repeat requests where I'm asking it to do the same thing multiple times. Some stuff about the schema I'll let you read that on your own time. This is a little bit about that entity resolution where that's like the what where it's detecting people and the entities in the system. you know, you know what those are hopefully. A bit about the table like where stuff is stored. So there is the core memories table. There's the session recalls table that links the memory to when it was surfaced so that if I refresh the page it will stay there. But also down the road I can look at a session and see actually I think I had a version of it up here where if we go into cloud sessions you can see in a session that has memories how many it has. So they're linked in in both directions. that's for there session recalls and then memory feedback that is storing the thumbs up thumbs down within the scope of a memory being included in a session. So and includes the query that surfaced it. So again all of that voting on whether or not a memory was useful is in context and that context is stored along with the memory feedback that I give it. Scopes queries here is the smart memory retrieval. So two main hooks that we're using in cloud code. There's the user prompt submit. Obviously, when I hit enter, it is going to do two things at the same time. It is going to pull out entities. So, an example, what events does Indie Hall have coming up? It determines that Indie Hall is an entity. It's going to match that against our internal databases. And if it finds them, it's going to look for other memories that share the same entity, right? And so, it's the existence of an entity that tells the system, hey, there's probably more things. Go look for them using that term. And then the second part and actually and actually that before I move on to that it brings all those back and it still runs a semantic similarity filter on that because I found that it was anytime I mentioned Indie Hall for any reason it was bringing back things that were not relevant to the conversation. So, it's going to bring back all the stuff related to Indie Hall, but then it's going to look at the semantic meaning of our conversation and the semantic meaning of each individual memory that has the word the entity Andy Hall in it, and it's going to drop anything that doesn't match the context or the meaning context of what we're talking about. And that got rid of a lot of junk that it would be kind of like I mean, honestly, it is kind of like the way when you're you're talking, you hear a trigger word in in conversation, your brain goes to that trigger word instead of what you're supposed to be paying attention to. That's what was happening here. And I was able to basically program that out. Program out that ADHD kind of response of like, ooh, shiny thing that you just reminded me of. It doesn't do that. It stays in context. And then the second half is the pure semantic search, which is if there aren't any entities, which is not uncommon. It depends on what we're doing. It just does the semantic search. And so it will generate an embedding in real time for whatever I posted, whatever it posted, and it generally looks at like a window. So it's not if I post a message, it's not just looking at that. It's looking at that plus the last three to five. There's a few different ways that it decides how much to look for. It generates an embedding on that and then throws that embedding against our stored embeddings in from our memories to find things that line up and again has to match a minimum threshold so on and so forth. There is a layer in here that looks for intent type keywords that will boost some of these elements which you know for example if there's keywords around mistake wrong or error it is going to give a little bit of extra weight to a memory that might be a correction or a gap whereas in another context it might drop those because they're below that minimum threshold of worth retrieving and sticking into the the chat itself. the re-ranking algorithm similar like this is a piece that I'm I'm basing modeled this off of best practices that I would say I found but I told it go search quads pretty like anthropics best practices on this I you know shout out you know folks that I learned from like Matt PCO and like what is Matt's material on this and base it on that and so you get a sense of how it's like waiting and scoring and and boosting based on those things there's [snorts] a the floor again the the minimums are adaptive and then also how my positive and negative feedback boost or degrade what the where a memory will will show up again next time. The second thing is it also after a tool use is it after claude reads or edits files specifically in the personal data directory. So it's not doing it for the entire project. Can catch things that I didn't necessarily say, but maybe it did. So for example, it touched a bit. If it's going to go edit a file, it should load what we know about touching that file in the past into the chat so that it doesn't repeat mistakes or maybe is more consistent in the development patterns that it uses, in the templates that it follows, in my preferences for how a particular action goes, those kinds of things. So in this case, you know, one of the things that gets updated the most in the files is my relationship files. So every time cloud code touches a relationship file, that generates memories related to what changes are being made in that file. So in the future, that is easier to recall. And then there's some stuff in here about the memory u the UI the memory lane how that works the embeddings in infrastructure and then some of the decisions that we made along the way that that got us here some collection of examples and test scenarios that are cool to go through and some real examples that it pulled out. So, and actually, you know, again, I didn't generate this document. I asked the Andy, I asked my assistant based on everything we built in the system, put together a document. I've checked it against most of our things. One of the things I didn't tell it to do, but you know, sometimes it anticipates me in ways that I don't expect, a checklist of what things you you would need to create something like this for yourself. So, again, all the more reason for me to share this. I'm just going to put it in a a gist so it can be easily shared around perhaps by file references. That's about it. So, you know, that's that's more than enough for me today. I've been working on this for a few days and is really starting to work well. I'm most excited to see what this looks like when of the memories look like my day-to-day. I also have a back fill script that I'm gonna probably run overnight and try and generate memories on stuff that I've done over the last few weeks besides build a memory management system for an AI assistant. So that this looks a little more diverse in terms of what it can represent. But I wanted to share progress on this because folks seem really excited and interested in how this worked and happy to answer questions and stuff like that as well. So keep the good stuff coming. Anything you build based on what you learned here, let me know what it is. Happy to share. And for folks that have been asking about open source and sharing, my my my brain is like in in in one of two ways. One is for many of you don't know, but I've been doing like education for not explicitly developers, but like technically minded people and creative people for a long time. So I could see doing some kind of like workshop or clinic where we build a version of this enough to get you started in some of these core concepts and and then you can be off to the races and building your own and maybe a little community of people building them perhaps. I don't know. And then the other thing is I could see this memory system if it if it continues working well. This feels like the first part that is not so deeply ingrained into my personal workflows. There's some parts like the entity stuff that is more. But I feel like this is the first part that feels very abstractable. Maybe as a cloud code plugin or or something like that. Whether we open source it or make that available as a product. I'm not sure. I would like to open source as much of the the the learnings as possible. And if we can if I can get to a point where I can extract the useful chunks of code that that I don't feel like are putting myself or any of the parts of my system at risk, I I want to share more of that stuff as well. Yeah, that's so curious to hear what youall think and what questions you'll

## Minhas Anotações

