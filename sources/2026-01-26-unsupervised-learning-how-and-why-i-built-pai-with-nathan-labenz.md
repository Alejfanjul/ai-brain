# How and Why I Built PAI, with Nathan Labenz

## Fonte
- **Tipo:** video
- **Autor:** Unsupervised Learning
- **URL:** https://www.youtube.com/watch?v=vvXC7sqso4w
- **Duração:** 121 min
- **Data original:** 2026-01-26
- **Data captura:** 2026-01-26

## Conteúdo

Kind: captions Language: en Okay, cool. Let's do it. Daniel Beesler, founder of Unsupervised Learning and author of PII, Personal AI Infrastructure. Welcome to the cognitive revolution. I'm excited for this conversation. I think it's very timely in the sense that obviously the world is waking up to the power of Claude code and now we've got Claude co-work mode for desktop as well. And so everybody's kind of like, oh my god, you know, this is changing my work in this way, that way. I'm, you know, I'm creating whole simulations of things that I previously just thought about and I've got, you know, memory palaces that are now like not just in my mind, but are actually, you know, in durable mode on computers. And you've been a pioneer of that over the last couple of years. I've um certainly been an AI obsessive for uh those same that same time frame, but have not gone nearly as deep into the personal AI infrastructure world as you and and other pioneers have. So, I I'm really looking forward to just picking your brain as I start to play catch-up a little bit on this dimension and definitely think that there's going to be a lot to learn on on that and and some other fronts as well. Maybe for starters though, um you've got a background in cyber security. You've worked at several big companies along the way. Now, you're independent and doing a a handful of different things. Want to just kind of tell us like a little bit of what your uh portfolio looks like today and and how we should kind of think of the different activities that you're known for? &gt;&gt; Yeah. So my background is definitely cyber security. That's what I did and I'm still doing. That's what I did for my whole career starting in like uh 99. I started getting into AI at Apple. I joined a machine learning team there. Was doing a bunch of stuff with machine learning and security there. So I got exposed to AI, I want to say probably around 2016 or so. And then took the job at Apple in 2018 and got more exposure there. and kind of been thinking about it for a long time, but it wasn't until I went independent in about 6 months before chat GBT actually. So, great timing. And then ChatGBT came out in, you know, late 22 and obviously I kind of hard pivoted, not getting away from security, but just seeing security as kind of like uh embedded inside of AI. I kind of see AI as like a container for like magnifying everything else that you're doing. So, um, my main focus now though is basically trying to help humans and companies, mostly humans, to just be able to adapt to what's coming. That that's that's kind of the main thing. So, I do a whole bunch of like open source stuff. You mentioned the Pi project. That's probably the biggest one. I've got another project, open source project called Substrate. And all of it is just trying to just kind of like move humanity forward. I I feel like the place that we've been at all this time has not been a good place. And it's only after like it starts getting disrupted, you know, people are like, "Oh, AI is going to disrupt our jobs or whatever." But right before this happened, everyone hated those jobs. You know what I mean? It It's like everyone knew that this was a bad way to live. One of my favorite metrics is how much you dread Monday. And one of my favorite metrics for what a good life looks like is do you look forward to Monday? And I think going by that metric, we haven't really been happy with corporate jobs for a very long time. So what I'm trying to do is figure out what does it look like to have a better version of like a human future and obviously using AI to like sort of power that. &gt;&gt; I love the starting point that just reminds us that most people didn't and and frankly still don't love their jobs. I think that is one of the weirdest bits of sort of clinging to the present or you know some sort of cope or whatever. It's it's a very strange thing to me. I think it obviously correlates strongly with the fact that a lot of people who are in AI professionally are very privileged in many ways and one of the great privileges that they maybe don't even realize that they have is that they have employment that they find intrinsically valuable and motivating and to some degree would probably do some of the same things even if they weren't being paid to do so or didn't need, you know, to to work for money. But I think that that is just not the case, you know, for the large majority of W2 workers in the economy today. And I think we would do really well to remind ourselves of that a bit more often. A couple things there I wanted to kind of double click on that you said. One is the just what's coming. So I want to have you unpack that as you see it. Obviously people have radically different understandings of what's coming. everything from, you know, still outright deniialism, which I think is increasingly kind of discredited and and can sort of be ignored, but there's still the sort of more, you know, credible version of like AI is normal technology. And then we've got, you know, people thinking the singularity is like very near. I'm somewhere in the middle, but I think I'm definitely more toward the latter. The other thing that I thought was really interesting there was AI as a container for security. I don't know exactly what you mean by that, but it does strike me that that is in contrast to a lot of what I see going on in the AI safety and control space where the idea is like we need to put AI in a box somehow and so like let's develop all these security measures around it whether that's formal verification of containers to you know keep them sandboxed or you know all sorts of other AI agents checking each other's work or what have you. Yeah, let's start with what is it that you see is coming and then we can go into the sort of u way in which AI and security relate to each other and security relate to each other. Yeah, I mean I think what I see coming is is largely the same as like a lot of people not not everyone but a lot of people are saying how it affects the balance of capital and labor right so it's like what happens when most knowledge work jobs you know robotics is a separate thing who knows how how long that follow will be but I don't think it'll take too long um after AI but essentially labor labor gets massively diminished so then ownership matters a lot more all right and then the question is okay, cool. We've done all this productivity. Sounds amazing. You can now make a thousand times more stuff for 1,000th of the cost. Who's going to buy it? Right? Because traditionally, the entire system has been built on this concept of you spend your wages to buy things and then some people make things and then the cycle goes round and round. What happens when that fundamentally breaks? So, that's the the main change that that I'm kind of worried about that's going to break the status quo, but at the same time, I'm happy that's going to happen. I'm not happy about how it's going to happen. I think it's going to be disruptive and a lot of people are going to get hurt by it. And that's kind of the whole point of what I'm trying to do is like ease that transition if possible. To answer your question about the security and AI thing, I think it's a great question. There's no doubt that AI is creating a bunch of security problems, but here's the way I think about this after doing all this consulting all this time. A big part of security problems, I would argue one of the major problems is actually that people don't know what's going on. There are too many things happening inside of an organization. New products are being developed. Leadership has no idea. Things are being shipped to production. Servers are coming up and down. Ports are opening up. Applications are opening up. New APIs are being presented. Software is decaying and becoming vulnerable. And all of that is happening at a speed at any size of company, like any decent size of company that you just can't human keep up with. Even if you're logging all this stuff, there's nobody to look at the stuff. Like there's not enough people. Let's say you have a 100 people and you're like, well, you know, we really want to take this seriously. Let's let's increase our number of people to a thousand people, which is not going to happen in any security or security is not the priority. But even if they did that, it still wouldn't be able to look at most of the logs, at most of the changes because things are just happening too fast. So the unique thing about AI is it with the whole agent stuff and more importantly the ability to encapsulate an explanation of what we're trying to do easily form our goals and align our projects and our work actually with those goals. This is a thing that AI can do all the time, right? It could be doing this continuously. So it can help with planning inside of the company. It can help a security team for example or an engineering team explain to management and to other teams what they're actually doing, right? And usually these explanations are come in the form of like these big presentations. It takes dozens of people or hundreds of people in the organization, not even hours, more like days or weeks or even months to prepare the next plan to present to other people. And in the meantime, all those plans are changing from the top. So you have like this constant state of churn and just old information inside of organizations that fundamentally is causing a lot of these problems um with you know uh being able to efficiently manage the company and definitely secure it. So when I say AI kind of contains other things, it means that all the things that I I think are required to run a company well and to secure a company, they get easier when you have more access to the data and you can instantly produce narratives of what you're trying to actually accomplish. Basically, it removes the opacity of other orgs. It removes the opacity of the top explaining what the vision is and giving it down lower. the broken state of that communication is just the cause of so much trouble. &gt;&gt; Yeah. Two major threads there. So there's the question of like how do we defend the role of labor and for how long can we defend it and then there's this whole security thing around and you kind of even started to expand you know beyond security I would say to just like organizational dynamics in general there. Um but you know certainly anybody who's uh dealt with server logs you know knows that you're absolutely right that there's no way to scale human time and attention to read all of the server logs. So two organizations that are coming to mind you know other conversations that I've had and and hope to do full episodes with before too long. One is Workshop Labs. You may have seen one of the founders there, maybe two founders there, wrote the intelligence curse and they're, you know, working toward a similar goal where they're like, how can we defend the bargaining position of labor as long as we can? And it's a good challenge to me because I I feel like I'm and this this may be something worth interrogating a little bit in terms of possible difference between our worldview. I feel like in the end, again, there's a lot of cope going on, right? I mean, I look at somebody like Tyl Tyler Cowan, who I respect tremendously and, you know, have read his work for literally 20 years now. I looked back recently and I think the first mention of zero marginal product workers, ZMP workers he's called them, um, dates to like 2010, maybe even a little bit earlier than that. And it was kind of a financial crisis, you know, mortgage bubble bursting sort of thing where all of a sudden, this is, you know, fairly typical in recessions, all of a sudden companies look around and they're like, "Okay, we got to, you know, get by here with less." like who do we not really need? And they don't tend to do that sort of thing because it's painful in all sorts of ways until they're really forced to, but the financial crisis kind of forced them to. And then what seemed to be discovered in a lot of places is like, hey, we could actually basically do the same thing with 10% fewer workers. And you know, I don't know if Tyler coined the term ZMP workers or not, but he was certainly, you know, blogging about it quite a bit back then. Fast forward to today and he's like, ah, you know, don't expect the labor share to go down all that much. you know, there's going to be various reasons it'll sort of rebalance out. And I'm kind of like, I don't know, man. It seems like we're already at a place where I'd rather work with clawed code in many many cases than like hire a junior developer. I'm not, you know, I think it's still very much debated like how much that's hitting aggregate statistics. And you know, we'll kind of only know that in the rearview mirror, but I have a very hard time imagining a world where the majority of people don't end up in a ZMP situation where, you know, and this also kind of goes to like what you're talking about with organizational dynamics and speed and, you know, Doresh has put out some good essays on this and I think like a Jay Catra has also philosophized quite effectively in terms of like as the volume and the speed becomes so overwhelming like kind of only AIs can handle it. So I guess if I try to boil that down to a question for you, how AGI filled are you like how far do you think this goes over the next couple years? And if you imagine the sort of waterline rising from, you know, maybe even before AI in 2010, it turns out like big companies didn't need 5 to 10% of their people. How high does that go? Like to me it seems like it clearly goes to a majority of people that are just going to have a really hard time contributing in the sort of fully realized AI enterprise of the future. and maybe we still have executives because we want judgment or decision-making or whatever, but like there's not a lot of executives. So, I tend to kind of come to like an end state of well, we're going to need a new social contract, you know, we're going to need a UBI. And then obviously it becomes a huge question. Well, how do we get there and on what timeline and what does that transition look like? And I don't have good answers. I kind of like often sort of wave my hands and say, well, we'll have to figure that out. We have a lot of time to figure that out. But anyway, I guess yeah, how how far do you see this going? How much of like the current labor force do you think is like long-term defensible? how much can hold up, you know, how how many people do you think ultimately are have a place in the sort of fully realized AI firm of the future? &gt;&gt; Yeah, I I think to operate in the current system, very few people will survive that that current system and be useful inside of a corporation. And and here's the way I I frame this, and it's kind of extreme, and it's a little, I guess, anti-worker or whatever. That that's definitely not my intention cuz I'm trying to get us to the stage past where everyone is much happier. But the way I think about this is the baseline for actually passing what is required to replace workers is extremely low. If you just think about what most knowledge workers are doing, we already talked about they're not happy in in most cases. They kind of dread Monday. They they're not happy going into work. And the work that they're doing most people I I would say most most workers is very sort of wrote and it's sort of just like you know you've got to get the email you got to summarize the email you got to write the report you've got to look at a number of different reports and create another one right and it's like if you look at the dead center what AI is good at it is so like covering of like all these many of these jobs so I I don't think the bar is very high at all I think gets extremely low, especially because the workers aren't really trying. This is their job. This is the thing stopping them from doing life. They they are literally just trying to get through the day. At the same time, they're being onslaughted by Game of Thrones politics constantly. It's just a hostile environment. So, it's not like people are coming to work and saying, "Wow, let me just unlock my creativity and let me be maximally intelligent in a way that's going to compete in some way with AI." So I think the bar is extremely low for passing what an average knowledge worker does in their jobs which is you know of course hundreds of millions of jobs. I would say on the other side this is kind of an extreme way to think about this but I think it's it's valid. I think for most companies the ideal number of employees is zero. I I think that's always been the ideal number. So the way I like to think about this is like if I had an ice cream stand I wasn't trying to scale. I wasn't trying to do anything like that. I just had my truck and I had my ice cream and I was selling the ice creams and I was making tons of money or whatever. I was making $500 a week and it was I could live off that. People could not pick at me outside and say, "Why haven't you hired me?" Right? Because I don't have any employees. It's just me. I go out on the ice cream truck. I make the money I want. That is what most companies wish they could do. They wish they could do all the work themselves. We literally hire people. And this is so weird. It's just like stuck in our brains. The reason we have a labor economy is because the people who came up with the company or the idea or the product, they can't do the work themselves. If they had that many brains and hands and could live in multiple places, there would be zero employees already. So, a way to think about this is AI is about to return to a more natural state of everyone does their own work. Everyone literally does their own work. You come up with an idea, you spin up a whole bunch of agents, those are your employees, air quote, and they go and do the work. So if someone says outside, hey, why haven't you hired me? It's like, what do you mean? Like I'm doing the work myself. Everything is fine. Why would I hire someone extra? So I feel the combination of those two is just really bad for the outlook for human labor in this traditional corporate sort of structure. &gt;&gt; Yeah, that sounds tough. I guess, you know, one thing that obviously we should give the the sort of skeptic their due at least in terms of a one follow-up question there. Why hasn't that happened more than it has already? And I'll confess that like, you know, I'm not a super forecaster, but I have done some of these like forecasting exercises where, you know, a year ago I predicted a bunch of stuff where it was going to be today. And I would say I always kind of overestimate how much disruption at least over the last like 3 four years. You know, I I think I've consistently overestimated how much disruption we would see in the next year. I think I've had a better sense of like where the capabilities would go. Probably overestimated that a little bit as well, but not much. But I've much more so overestimated, you know, like how different will the world be a year from now. Um, so maybe I've just been kind of wrong as to where thresholds are that are that are really the key thresholds. But honestly, I do think even going back to like 2024 for sure, from the time you could basically fine-tune GPT4, it seemed pretty clear to me that most organizations, if they were determined to, you know, really do this and go just take a systematic look at like how are people spending their time, what are the tasks, you know, you know, where are all of our resources being spent and they just started making a priority list and trying to get AI to do those tasks, then I think they could have got there sooner and I'm pretty confident in that view. But then it leads me to like okay well now we're you know here in early 2026 and like it's sort of the old uh you know computers thing again that we see it everywhere but the macro statistics. How do you make sense of that? &gt;&gt; I make sense of it because I think the value of AI is actually in the scaffolding more so than the models. So what the model is capable of doing doesn't really matter if it's not inside of a scaffold that allows it to take inputs and produce outputs that are actually useful. This is why clog code has gone crazy because it is the best scaffolding system, right? The difference between Opus 45 and you know the best Gemini model or the best open AAI model is not much and the other two are better in some ways. In fact, the open- source models are very close. It's not anthropic that's blowing up. It's not Opus 45 that's blowing up. It's cloud code because it's scaffolding. And to answer your question about why this hasn't happened before, even before AI, but but in the previous, you know, three years of AI, in my mind, it's because the average knowledge worker job is extremely general. So when they come into work, it's like, you've got to check all these emails. Oh, but you have to watch this video because it's mandatory uh secure code training. Oh, but also there's this fight going on with your boss and this other uh person, and you've got to talk about that. Oh, turns out you have to have an HR meeting. Oh, actually, corporate goals just changed completely. Now, we have to redo all of our work. So, we're not working on that project anymore. We're spinning over to this other project. So, in the course of a week or a month or a year, human workers are being asked to do like these vastly different things. even in the course of an hour, you might have to check emails. You might have to fix your email. You might have to watch a training course. So there there is not a scaffolding system that exists right now that would allow an AI to do all of that. It just wouldn't be possible. So you would have AI that's really good at the coding part. Maybe it's really good at writing reports, but how is it taking all those inputs in and producing the output in the same way that a human worker can? They it can't, right? And that's why we don't have like giant armies of AI employees out on the market yet. And here's what I'm very worried about, and this is why I think 2027 is the year for AGI in in my definition, which is the ability to replace an average human knowledge worker. The question is when will it be when will the scaffold and we just saw co-work come out? Is that what they called it? Enthropic co-work. We just saw that come out. That is a scaffold system for doing broad tasks at work, right? It's actually for more general tasks as well, but that is the type of thing that somebody can build an AI product on that actually replaces human workers because now all those weird general things that are happening inside the company, those are just one-off tasks. And and here's a really crucial point here. It doesn't matter for the replacement of human work and the disruption of the labor economy. It doesn't matter if it happens with, you know, the wizard behind the curtain, which is actually doing a whole bunch of narrow AI, but it's able to do it for all the tasks that an average worker does, and it's just being handled seamlessly with the scaffolding. Doesn't matter if it actually does it way better than an average employee, right? So, when when I talk about AGI, I'm not talking about like what does archive think, you know, the the technical research papers. I I think it's cool that they're going down that path and and I can't wait to see what they do if they create a truly AGI ASI intelligence, but what I care about is the humans. I care about who's getting fired, who's getting not hired. And I think the way that happens is through a scaffold that can actually do their work better than them, which I think is going to look a whole lot like Pi, which is the project I'm doing, Claude Code, which is what Pi is built on, and co-work, which uh they just built with Cloud Code. They said they built it in a week, and they there were no humans involved. Cloud Code wrote all the code. &gt;&gt; Yeah, Anthropic is in many ways an organization to watch in terms of a leading indicator on what the future is going to look like. I understand they're like not really hiring any junior roles anymore pretty much at all and the execution time on some of these things is is getting extremely impressive. We've seen some of that from OpenAI as well from time to time. I think Codeex they said that they did in like six weeks and that's you know like two generations ago of of models um powering it. But I mean those are pretty ambitious things to spin up in a in a remarkably short period of time. I guess the the key thing there and I share this intuition is that I frame it a little bit differently I guess but I think we have a pretty similar intuition there where if you can get over this threshold of the drop-in knowledge worker and the interface from the boss to the work getting done can basically be swapped out from you talk to a human to you talk to an AI system that might be three AIs in a trench coat or 57 AI in trench coat or whatever. Um, but as long as it can sort of handle with sufficient generality kind of whatever you might want to throw at it in a similar way to whatever you might want to throw at a person and not get like boneheaded falling over responses back, then it seems like you get to a point where people have very just obvious and they're, you know, they're not going to miss this, right? The obviously the economic incentives are very strong to not miss this opportunity as it really starts to work. Then people are just going to have like behind door A you can hire a human or behind door B you can hire an AI and the AIs obviously have so many advantages in terms of breadth of knowledge you know 24/7 availability immediate response like cost you obviously there's just like you know that just to name a few important ones and so it does seem like we both kind of share a threshold model where when that flips it could flip really fast then we could be in a world you know in a pretty sudden way where there really just aren't junior jobs in the way that there used to be and I think it should be said too like even our fairly highly educated people like fairly high status in society you know may just find that like an AI can do what they do and obviously then we have kind of a a crisis on our hands so give me a little bit more detail on like how there's kind of I guess a couple dimensions of this one is like what are you building this gets to the pi project and you know we can unpack that in it's almost a fractal way because there's a lot of depth to it and then the other question is kind How does that translate to a world where you know some significant share of people can actually maintain some sort of market power, some sort of bargaining position, you know, some sort of ability to be economically viable in the face of, you know, the the transformations that might come to corporations. &gt;&gt; Yeah, if I can let let me add something real quick to to uh the previous part of what you were saying. Basically, I I see AGI as being a product release as opposed to like a model release. So, I think some company is going to come out with whatever virtua worker or what whatever they're going to call it and it's going to be a cloud codel like system that can basically do this work. And I think the way to know if it's working is if they are actually deployed inside of companies, not proofs of concept, they're actually deployed in companies. And and here's the standard which I think Carpathy might have mentioned this or somebody I was following a while back mentioned something like this. They onboard. They show up. They're in the cohort with human employees. They go through the onboarding. They watch all the videos. They do the training. And then Monday morning they show up and they're on the all hands with the the team manager. And the manager is like, "Yeah, here's what we're doing." Blah blah blah. You know, uh Sarah's over here, Robbie's over here. You know, Chris is over here and we're going to assign work. you know, how was your weekend, right? And the AI says something, oh, I read some books or whatever, like whatever it's going to say to like try to act human and it proceeds to take work from the the manager and do the work and return it. And importantly, when the manager says, "Hey, our goals have changed. You're not doing that work anymore. You're doing this other work." It needs to be able to pivot just like a human does. So, this is a scaffolding AI product as opposed to computer science. You know what I mean? Obviously, there's lots of computer science underneath, but to me, this whole encapsulation is as a product, which honestly could happen this year. Um, I'm guessing 2027, but I mean, I could be wrong. Like, it could be 28 or 29, but it it just seems inevitable that that's what in my mind, according to my definition, AGI looks like with replacement of workers. &gt;&gt; So, I was going to start to get into what you're building to help people carve out their own niche for themselves. And you know, then I think there's kind of still plenty more big picture questions, too. But maybe let's get into a little bit like, okay, you know, so we've got this problem. Corporations are going to be like extremely AI. Jobs are going to go away. What does that leave for people and and what are you building to help them defend or seize uh what I don't if it's defend or seize? Kind of both. Seize and defend the um opportunities that remain. &gt;&gt; Yeah. So the the way I frame this is that I I don't think most of humanity is activated in terms of a very specific thing that I'm talking about here which is I use this heristic of like a visiting alien with a clipboard. So the visiting alien shows up and they just go to random people on the planet. You know a billion random people all over and they're like hey so uh who are you? Uh what do you do? I' I've been all over the galaxy 19 galaxies actually and I just interview people like what are you about? and they're like, "Well, um, I'm a, um, accounting specialist. Um, I work at, you know, so and so company. I provide this sort of thing. I do this. You know, I check the spreadsheet. I update the thing. I send the report." They're like, "No, no, no. Who are you? Like, what do you what are you about? What are your beliefs? What what do you think is wrong with the world? How do you plan on changing it?" And they're like, "Yeah, I I don't know. That's that's for special people." It's like, well, do you do you have ideas? Do you talk about your ideas? Do you put them out into the world? It's like, oh, no, no, no, no. I I'm not I'm not an author. I'm not a YouTuber. So there's a default sort of state I think that's just is it's no one's fault. It's just like the history of humanity where people have been taught that there are special people who have podcasts and have ideas and write them down and think that they are worth sharing with others and then they're the regular people which are the 99%. And our entire education system for all these whatever thousands or hundreds of years has taught us that your goal is to get a job from one of the 1% people and you're a worker. And this mindset has basically shut down like the creative capability of like the entire planet rounded rounded down to like zero because there's very few people who are currently on YouTube who actually believe that they have something worth saying. So my whole plan and I have no idea if it's going to work. It's just too sad to think about it not working. So it's the only reason I'm running full speed towards it cuz I'm like well this might be possible to help bring about therefore I'm going to try. And that is we have to activate people. We have to turn more of the 99% whatever the numbers are right might be 99.999 or it might be like 95%. whatever we have to turn more of those people who think that they are just workers for someone special to realizing they also can be special. They also have ideas and I have seen so many pieces of evidence of this over my life where you can activate somebody by just believing in them by just telling them that they are capable by just saying hey you realize that that was a really cool thing you just said like have you ever written that down it's like no no no that's nobody would read what I would say right I mean how how many people are like believe that they're they're just mothers they're just moms right they're they're just providing this and you're like, "Hey, wow, that was a really smart way you just said. Have you ever shared that with anyone?" Well, who wants to read what I would say? So, here's a sort of theatrical way of saying this. Imagine that planets from this alien has visited have stats hovering over, you know, they can see a stat for creativity activation for planets. And when they're scrolling through their phone looking at all the different planets, the trillions that they've looked at, when they scroll over Earth, it says 0.0013, that's how much human activation of creativity has occurred on the planet. Right? So that is massive opportunity. And my favorite version of this is having a persistent tutor, a persistent assistant. And this is a little bit in the future, but we we'll get there. a persistent a persistent tutor that is working with this person, letting them know like not going super sycopantic, right? But letting them know, hey, look, you do have ideas. You do have value. You are smart. Hey, do you want to learn more about that? And just always being available from a young age. And obviously you have to be careful with this stuff early on, but having children be able to be tutored both in mindset and believing that they are capable of things, but also enabling them with tons of knowledge, right? So I feel like that would be a huge lever. I feel like obviously we need to fix like society and the way governments work and all that kind of stuff which will be difficult because a lot of times the the challenges are very real. It's like well my parents are working three jobs each. They don't have time to nurture me therefore you know bad things happen right so we have to fix all of that at multiple levels. But I think AI presents an opportunity to encourage people, especially children, but really anyone to unlock this this power within themselves. So, sorry for the rant there, but all this to come around to Pi. So, Pi is designed to be a customized personalized AI system where um so I've got this project called TLOS which basically it gathers from people what are their goals. It basically does this alien interview. Who are you? What are you about? What are your goals? What what do you think is wrong with the world? It actually starts with problems. Problems is number one thing. What do you believe are the problems in the world? And then okay, what do you want to do to change that? Um what are your obstacles to doing that? And it could be personal problems. It could be like, well, I'm too heavy. I've never been able to lose the weight. I have low energy or whatever. But this scaffolding of problems to challenges to projects, this system like basically can tell the PI AI what it is you care about and what you're trying to accomplish. And at that point, the AI spins up with all the scaffolding to help you to help you with meal planning, to help you with like encouraging you, to help you find other artists, right? Because this I'm not trying to build a product for tech people. Tech people are already techy, right? This is not about coding. This is about enabling a human to be better at what it is that they want to do like to help them activate their full self. So practically that means capturing their goals, their current capabilities where what they would like to learn how to do. And it also starts with mapping out what do you normally do during a day, right? And that's in work, that's in personal life. So for me and you, it's a lot of writing. It's a lot of writing and thinking. And so my workflows are many of them are largely focused around that. So I could capture an idea. I just wrote a replacement for buffer. So I could go from an idea to red teaming the idea, having a council of AIS debate the idea, fight with me about it, and I'm in here editing, right? Making the adjustments on the fly or I'm doing it with dictation with uh shout out to Whisperflow. And um I end up with that and now I say cool, put it on X and LinkedIn. And it's able to do that. So this workflow which I see as extraordinarily human, the most human thing you could possibly do, which is like have an idea and share it with the world, that is now made extremely simple through this whole AI workflow. And it's all built into Pi. So I'm literally telling my DA, my digital assistant, Kai, hey, I had this cool idea. What do you think? Even better, I have this uh pennant that I wear. It's uh Limitless. So, I can go on a walk out by the bay and I could ramble off some uh half halfway stupid idea or whatever. I get back and I'm like, "Hey, go get that conversation that I just had. Let's work on it as an idea." And now I'm live editing because it pulled it from the API. So, I'm just like removing all this friction to being able to do more human things in your life. &gt;&gt; I don't want to get too bogged down in some of the things that we probably can't today no matter what we do. Uh and I definitely want to get into more of like the tools and the sort of practical stuff. I mean first of all I have to agree with your sense that of course the podcasters are the special people and totally that the socialization that we've put in place for society broadly is like on the verge of becoming you know it may have served us well for the last 150 200 years as the you know structure was what it was but it does seem like it's on the verge of really becoming you know a major liability for us because it does have a lot of people answering I think in the way that you describe. I'm a I'm a little less clear on and you can either, you know, respond to this or just kind of say, "Yeah, we'll see how it goes over time." But I'm a little less clear on like how many people really want to scale their agency or you know be like change makers in the broader world even you know given versus like how many would say actually no I'd rather just focus on my relationships and you know spend a lot of time having like the best you know VR perhaps mediated experiences that I can have and I guess it's sort of a production versus consumption question on some level like how many people would If you gave them a life of leisure and relative abundance and like, you know, all the time they need to focus on the relationships that they have, how many people would say that's not enough for me. I want to, you know, go make a difference. I'm not sure about that actually. It's it'd be very interesting to find out. And I do think it probably will be somewhat generational because either people who were socialized in the current way are going to have to do some quite challenging unlearning or re-education or you know it's going to have to come from like a next generation. Obviously a huge challenge here is like we don't really have unlike previous revolutions the industrial revolution I always like to you know remind myself took like depending on how you want to count certainly multiple generations the electrification of the United States you know was like a 60-year process from when electricity was you know Edison's first you know wiring up to when my grandmother in rural Kentucky got electricity as a young person that's literally a 60-year you know kind of three generation time frame and we don't have three generations today to uh you know to bring up, you know, people that are going to be AI native. So, there's definitely some like major open questions there in my mind and major challenges. And I'm not sure how much more time we should spend on it or if you have kind of additional thoughts that you would want to offer there, but &gt;&gt; yeah, I can add one thing there. I also don't know that number, right? I'm also agnostic as to that number. I do think it is a high percentage. I I think and here's here's the even more important point. I think it's worth trying. I think we constantly try to ping with that encouragement. I've hardly ever seen anyone who I try to activate in this way. Sometimes I try seven times over the course of, you know, 13 years or whatever and it bounces off each time. Fine. I'll I'll be back in 2 years and I'll try again. Right? So, it's fine if it bounces off. It could be that people are just so used to being in consumer mode that if you give them the option, for example, they're watching a Netflix show. They're like, "Look, I just want to watch Netflix. I just want to read stories. And you know, you ping them and you're like, "Yeah, but have you ever thought of a cool story? Like, what story would you like to read?" And they're like, "Oh, I would love to read a story about this or this or this." Well, guess what? In 2026, they're about to be able to write that story and publish it and become a famous author. That is super exciting to me that somebody could actually The first step, the most important step is that they they realize it's even possible. they stop talking negative to themselves in the sense that oh that's for other people. So I feel like these activations h these barriers to the creativity have to come down and and which is all part of this this marketing I'm trying to do around like activation but um it could be like it bounces off a lot of people that's fine I I don't know the numbers it's I think it's impossible to know the numbers but I I think it's worth trying. Yeah, that reminds me again of Tyler Cowen. One of his famous refrains is that one of the most high impact things you can do is try to raise the ambitions or aspirations of other people. And I totally agree. It's absolutely worth trying whatever that number ends up being. My buddy Gopal also says think less about what the number is and more about what you can shift it to. And &gt;&gt; totally &gt;&gt; that applies for so many things I think including this. Maybe just one more beat on the kind of big picture before digging in on the actual you know practical imple implementation side. So there's kind of I I just want to untangle a couple concepts. One is like I can I can create you know that I might have something worth saying. I might have something you know worth a kernel of an idea in my head that might be worth realizing versus reflexively shying away from that. That seems to me like it's absolutely worth encouraging. It's certainly part of, you know, at least some sense, some definition of a a life well-lived. And even if it's not for everyone, you know, to do work that expands people's option sets to include that, you know, seems like obviously good. Then there's the related but distinct question of is that something that people that can sort of sustain something like the current economy with something like the current social? &gt;&gt; Yeah. or do we still kind of need a fundamental rethinking of that foundation such that this sort of agency stuff kind of becomes in a way like its own form of consumption. It's maybe more of a creative consumption, but you know, I might write books or, you know, create my own whatever prestige TV series, you know, for myself or my family or a few friends. And, you know, maybe that's awesome. Maybe it's a great experience, maybe it's enriching, maybe it's, you know, still never it goes totally famous or especially if everyone's doing that, right? Time obviously is sort of the core re core constraint at some point like we can't all watch each other's prestige TV shows. So it could be awesome, but I do still wonder how much more work you think that can do for us in terms of allowing people to like earn income, you know, as a way to sustain themselves versus like kind of being another way for people to self-actualize on top of, you know, some different social contract base that we might need. &gt;&gt; Yeah, absolutely. I don't know what that looks like. I feel like I know some pieces of it. So I I think there's an opportunity for I did something about this like 10 or 15 years ago. Bas basically like if everyone is broadcast imagine like a LinkedIn everyone is broadcasting their capabilities. It's like I'm a trained dog sitter or whatever. So it's like you basically publish via like a Damon or something uh put it out on the network that you need this thing done. I need this tile replaced on my roof. I need a dog sitter and I need someone to teach me Spanish or whatever. and that beacons to the people who are available who have those skills and so you have like this web framework thing that just links people with desires and and capabilities needs and capabilities right so I think that is an opportunity for like you know a future techoriented alternative to an economy I I don't know I I don't feel like I'm smart enough uh in this area to to know if that's enough I I feel like it's definitely not practical as like an alternative to what we currently have. We can't just like jump to that. I don't I don't see how that works. I don't see how people pay their landlord. I don't see pe how people just pay for their groceries using this. So, I feel like there's probably got to be some sort of, you know, agreed upon shared system that is like paying people to survive. So, I I don't see an alternative to UBI needing to happen in the next few years or at least, you know, 5 to 10 years or whatever. I I think that's probably going to need to happen. I'm I'm guessing around 28 29 there's going to be just a raw like demand for for UBI because things will start falling apart. But I do think this this techbased exchange of need and capability will be one of these layers. Uh ideally it would be the only layer but I think that's so far in the future even if it's possible that it's not really worth practically focusing on. What I'm mostly focused on is getting people where they are broadcasting those capabilities. They are broadcasting those ideas. They do believe in themselves believe they have something worth sharing and producing and uh you know that's valuable to others and they're actually whatever they're paying in like uh Dr. like Wuffy, like reputation score, points, whatever they're paying in. But I think there's likely to need to be a more practical transition to that which involves Yeah, you're actually receiving money to survive and then maybe this other layer is like on top of that. &gt;&gt; Yeah, I think that's probably I mean I think that's very close to kind of the best ideas that I've come up with so far as well. I can certainly see and it it does feel exciting to imagine a kind of second level economy of like highly bespoke, highly personalized, you know, potentially highly local services. You know, I I for whatever reason my mind always goes to like the sort of murder mystery dinner, which I've never even done one of those, but you know, this is something that's just like obviously a luxury, obviously the kind of thing that people, you know, create these sort of highly crafted Yeah. curated experiences for each other and that feels like it could be a great way for people to interact and express themselves and you know have status and value and you know have some exchange but yeah it doesn't feel like that can sort of be the foundation for not everybody can like you know get their um get their calories certainly from that kind of activity. So yeah, I think we're we're pretty much on the same page there. And it's crazy how crazy this stuff is, right? I mean, it's it's what a weird moment in history where just all these things are sort of on the table for rethinking. Of course, some people, you know, don't believe that or don't don't recognize it. Um, other people think it's going to be even more insane, like we're all going to die extremely quickly, which I don't entirely rule out as a a thing to be worried about for the record. I guess on that, do you have a P doom or like what's your um existential risk story? &gt;&gt; Yeah, I don't know. I feel like I have lots of different pdooms and like I feel like they change a lot. I'm not sure how to think about that anymore. Honestly, I I've gone through all the literature and all the arguments and you know when uh Udekowski Yeah. when he he went on Freriedman for the first time, like I lost a lot of sleep that day. And yeah, I I think the chances, this is kind of another reason I'm doing this and and so focused on the positivity. The chances of things going bad just seem so high to me. In some ways, I feel like the most likely thing is no, not for anytime soon, maybe never. We don't get this future value exchange layer and all of that. Like the most natural tendency is like elites get extremely powerful with this really powerful AI. The other 99% kind of have nothing and they don't even care to look for it because they're so diverted by really immersive games, right? And then the governments mobilize and basically, you know, China and potentially US like they're just authoritarian regimes using this AI to control people and it it's more effective than it ever has been, right? So, I feel like that's a really easy one. Another really easy one is just everything just breaks and there's just chaos and then you have to sort of rebuild things after that. So, I feel like there's like this thin walking path where there's like chaos over here and it's just like, you know, really bad stuff. Then mostly it's authoritarian like control, authoritarian/ elite control and it's just all bad. And I'm like an emotionally sensitive person. So if like if I if I scroll that stuff too much like it it's not good for me mentally. So I literally am trying to lock on to okay break out of the mold of what is possible. Is there a path to possibly making this thing good? Go and build things that could potentially make that happen, right? Which is all the open source stuff. And then like try to get other people to do the same, right? And there's other people doing this already. and then just lock on to that and breathe it fully and just like and people will be like, "Well, you're not seeing the downside." Oh, no, no, no. I see the downside. In fact, I think it's probably more likely, but I can't live in that world. I I can't survive just thinking about how bad it can be, right? So, I'm not sure. The one that I think is least likely is like boom, ASI pops and it's like the cliche paper clips instantly. That one I don't see happening. I just see so much friction layers uh so many friction layers in between and stuff like that. So I I don't see that as being like one of our main risks. I think like an AI control would be more I I would say gradual and hopefully gentle, but it could still be really bad for humans. It could still lead to the extermination of humans or whatever. But I don't know. I I don't see, you know, 2026 or 2028 the ASI pops up and just destroys us. But I see much more possible and practical negative things that I I definitely want to avoid. &gt;&gt; Yeah, it's funny. I was an Iliaser reader, you know, way back when he was um on overcoming bias, you know, for the for the OGs that remember those days. And I do agree that the the sort of classic canonical paperclip model seems much less likely now than it did then. Certainly Claude is remarkably ethical and has remarkably strong character. At the same time, I do worry that like geez, you know, these frontier companies uh or at least a couple of them seem to be really keen on sprinting toward the automation of AI R&amp;D, which then like would, you know, I think would have to kind of raise your paperclip or sort of, you know, paperclips higher again because it doesn't seem like we have we have a pretty good, you know, loop right now that is, I think, making Claude pretty good like mostly, right? And even when it does sort of bad things, you can kind of squint at it and say, "Well, you know, it sort of lied there because the user said it was gonna change Claude's values to be bad and Claude wants to be good. So, how should I think about that?" You know, it's I can at least be somewhat sympathetic to Claude in in a lot of those scenarios. Or even though like autonomous, I don't think we necessarily want AIs to be doing autonomous whistleblowing, but you know, in that scenario, it had reason to blow the whistle, right? Like it was, you know, the hypothetical drug company was like faking data and reporting fake data to the FDA. So, you know, Claude is not wrong to object to to some of those behaviors. Nevertheless, I don't think we have, you know, for all that that's good. It doesn't seem like we are quite ready to like spin the AI automated AI R&amp;D centrifuge, you know, at maximum RPMs and expect that that thing will just kind of stay stable and stay in place. So, yeah, I don't know. It's I I also kind of find some of these things like can talk myself in circles. I don't want to force you, you know, I don't want to put you in a emotionally stressful position for talk about it. &gt;&gt; But just one one area there because it is like your professional background and expertise. How do you see cyber security playing into this risk, right? I mean, we or this sort of family of concerns. We've got AI could kind of go totally rogue and do something like extreme. We've got kind of gradual disempowerment where it's like everybody sort of willingly and rationally at each step like gives AI systems more and more decision-making, discretion, power, autonomy, whatever. And then next thing you know, you know, there's not really any humans in the loop anymore. And yeah, that might be like okay, but now the AIs are really running the show and we're just kind of along for the ride. And then somewhere in between is this like cyber security world where of course like you know AI seems to amplify all the threats. It also seems to you know at least have some promise for a sort of DAC uh you know infrastructure hardening or whatever. another episode hopefully I'll be doing before too long with a company called Asymmetric is literally just as far as I understand right now and I'm more to learn but they seem to be really trying to do the like log reading that you were describing earlier you know they said basically like what what happens when there's a security issue today in a company is like people go do forensics on it and they try to get down to a root cause but they only do that once harm has been done and now they're called to attention and they have to go investigate and so their idea is basically like well what if we just scaled cyber security forensics as much as is needed to like read all the logs all the time, you know, and try to identify these things before they actually become critical issues or, you know, whatever before harm is actually done. Anyway, that'll be an episode kind of coming soon. But where do you think we are right now in terms of I don't even know how you want to frame it, but like offense, defense, balance, like is cyber security about to become our worst nightmare or, you know, might we use AI to get it under control? &gt;&gt; Yeah, I think it's definitely a combination. My favorite frame for this is basically that the the game a as of probably last year, definitely this year and going forward is it's um it's the attackers AI stack against the defenders AI stack. That is the competition. So the the goal of the defending security team is going to be how good of an AI stack can they build to actually do this stuff. So like I I've been doing this whole attack surface management thing for uh you know decades or and and so many people have also been doing this. It's about do you understand your attack surface and with all these AI tools the attack surface is everything. It's like it's total knowledge of the company. It's total knowledge of every employee. Yeah. I built a thing that like it just it finds all employees and creates a psychological profile on them which allows me to write the perfect spear fishing email, right? And it's like, oh yeah, you you adopt dogs, therefore here's what this thing looks like. And I could also figure out, oh, you're also one of the people making this core product. Oh, it's also releasing a new version. Oh, it's also running on this platform that's vulnerable. This is all work that a red team could have done. But it comes down to like this concept of many eyes uh which was supposed to secure us all this time with open source. But turns out the fact that humans could look at something doesn't mean they will. And that's what that's the case with this this asymmetric thing you're talking about, right? With all these logs. The logs are there. There aren't enough eyes. There's not enough time. There's not enough attention. Humans need to rest. They miss things. So it's a matter of maintaining uh state. You have to understand the state of your company, right? And this is this is I think kind of the big picture here. If you understand the state of your company, like what is your profit and loss? What are your goals? What are your competitors doing? What is your infrastructure look like? What is currently facing the internet? What applications are you running? What stack are they running? What vulnerabilities do those stacks have? What just changed in the last, you know, 13 seconds while I was saying that sentence? Right? Faster and faster granularity. Oh, this person left the company. Oh, so and so joined the company. Oh, well, that person is extremely vulnerable to this type of social engineering. So now we're going to spin up this entire thing, this campaign to go after them to get access to the company. Prior to this, all this could be done by a high quality attacking team, a high quality pentesting team. Well, I'm thinking more like attackers. So like a really really skilled advanced persistent threat team, but they are very small teams. they're specialized in specific industries and verticals and they could only go after so many companies just because of the time. Now we're in a situation I mean claude code is kind of the model here. Pi is the model here where the attacker basically says look I'm an expert at going after these types of vulnerabilities. Spin up uh capability to do continuous recon to find all uh employees inside of a company produce psychological profiles. We've got another module over here that writes the social engineering attacks. Got another module over here that does the network attacks and the scanning. And this beast, they basically just put in a target and it starts hitting them and it spins up all these different modules and agents and it's constantly hitting you. Now, on the receiving side, there's only one way to survive this and to defend and that is you have to be doing the exact same thing. Like there is no game. You can't like, well, we need to hire smarter people in our company. No, that's not going to work. It's not going to be enough. The only thing that's going to work for is helping them improve the AI. Them helping the AI improve and get better because the scalability and the pace of change is actually what matters. All that to say, it's attackers spinning up better and better versions of cloud code, basically cloud codework or whatever. And I'm not saying they're only using that, but Anthropic did say that they've already seen automated attacks using cloud code being extremely successful, right? So these sorts of stacks attacking the planet, attacking the all these companies and then all these companies have to have a similar stack that's defending them. &gt;&gt; And that defending is the first version that I imagine is kind of that it's going and like sort of self-attacking and trying to find the vulnerabilities to then presumably patch them. Is there a better or sort of more comprehensive version of that that is like &gt;&gt; so yeah describe that for me. &gt;&gt; Yeah. Yeah. So in this world if the AI stacks are equally capable the defender will actually have an advantage because guess what the defender has actual access to AWS direct access to AWS. They have direct access to the network logs. They have direct access to all this stuff where attackers hopefully are inferring this from external signals. So hopefully the defender has a massive data advantage. So a big part of cyber security is just misconfigurations like it's not like writing special malware. It's just like oh I didn't even know that thing was still out there. Oh I didn't even know we still had that company. Like it's like huge like own goals. So the internal sort of agentic AI stack should be watching all of that stuff very carefully and really it's just a game of it finds it first. So it's doing this self attack. It it's monitoring all the logs. It's seeing all the configuration changes and it's saying oh well look that was bad. And you know you go back 15 20 years when I started doing this and it was like you would have weeks of a window. You better shut this down within a few weeks. Someone's going to find you. Well, now you know now it's down to hours and minutes, right? And pretty soon it's going to be eventually seconds and it is already in some places. But the attacker should have a disadvantage because they have to infer signals whereas the defender can just get it directly from the source. &gt;&gt; Wonder how you think that applies to the social side of social engineering. One thing that happened to me recently was so the company Send Grid, which is now part of Twilio, has this like email sending API. you know, was kind of I think it still remains like a market leader in terms of just high-scale programmatic email sending. Well, naturally, you know, if you can get access to somebody's send grid and you're a scammer, you know, that's a at least for a minute, you know, that's a really valuable thing to have because you've got like their sending reputation and so you can potentially actually hit the inbox with your scams based on the fact that you're hijacking somebody who's like, you know, maintained a good uh reputation in the email system and using their channel. So people aren't actually trying to hack into other people's send grids all the time. I got an email the other day that I don't know how personalized it was. You know, certainly the psychological profile part like it wasn't wasn't so great that I was like sure that they had profiled me, but basically what they sent was posing as Sen Grid and saying like we support ICE, you know, join us in supporting ICE, whatever, right? So naturally putting people into this kind of pissed-off state. They're like, wait a second, what? my email company is like taking a stand with ICE. &gt;&gt; Yeah. &gt;&gt; You know, this is going to get people inflamed. That's going to get people to click on the link if only to then go, you know, log in and cancel their service or, you know, go log in to try to register a complaint or whatever. I didn't click the link, but I would expect that there was probably a very prominent like give us your feedback, you know, UI there that then okay, now go log log in to Send Grid so you can give us your feedback and then of course you're getting pawned. So much of what you just described was like managing the attack surface on a technical level, but when I give somebody my password, you know, that that's a little bit of a different beast. Or maybe you think of it as kind of the same thing, but how do you think about the fact that we are just such juicy targets as humans, you know, maybe more so at least at a more mature state where the AI have gone and like, you know, closed up the open ports and, you know, fixed the misconfigurations. There's still like the, you know, human gets pissed off at a fake email and goes and gives their password away before they, you know, before cooler heads prevail. What do you think AI does for us about that? &gt;&gt; So, it's it's exactly the same sort of model of attacker versus defender AI stack. So I could easily right now I I could say hey and I'd have to be very careful with uh my relationship with anthropic here but I could say hey so based on all the history of social engineering attacks being successful and the fact that you have all these uh psychological profiles of this company why don't you come up with 16 or 36 or you know 128 really cool campaigns that would work against these employees or against Sangrid for example or find me a company and uh come up with a campaign that if you send it it out, it's going to produce outrage. But you don't even have to give it that much. You could just say, "Okay, you understand that outrage produces clicks. You understand that being psychopantic produces clicks. So create me 256 campaigns." And we don't have to pick one. We could say launch all the infrastructure to send the emails, launch all the receiving analytics to gather the data which includes the passwords which includes going and performing the attacks using those passwords including sending that up into the exchanges where you're actually selling the access and everything. So before this would be a whole bunch of attackers hiring very smart coders who are not going to get caught by the police are not going to talk about it and blab about it and get themselves caught and now it's simply that's a prompt that I sent into cloud code or open code which doesn't have all these restrictions right that is a prompt one prompt in 2 minutes and now I have 250 campaigns going off with different ways of attacking people through social engineering using completely completely different psychological tactics and they all spun up separate infrastructure and now a bunch of passwords are and access tokens are floating in. So it's just how quickly you can go from an idea of how to harm to actually making it happen. And and that's that's what's crazy. And on the defender side, you just have to assume that millions of agents are being pointed at you with all this knowledge about your company and about your infrastructure. And that's the assumption you just have to travel under. Sounds like there's going to be some uh spectacular hacks over the next couple years before everybody really gets that message. &gt;&gt; Yeah, I think it gets worse before it gets better. Yeah. &gt;&gt; Well, let's turn to more positive themes and uh finally get into pie. Maybe for starters, you've done a a little bit of this along the way already, but let's take a moment to just kind of share some of the stuff that is like magical for you to just kind of try to inspire me and others. And for context, like I said a little bit of this at the top, too, but I, you know, I use AI every day. I use tons of different products, but I mostly haven't, especially over the last couple years while I've been, you know, kind of doing the podcast, doing this like AI scouting thing. The thing I have prioritized most is learning. And then, you know, producing a podcast kind of is great in that some people seem to want to follow my learning adventure and learn with me. And also, you know, it turns out that you can actually make a living doing this, which is a a shock that I try never to take for granted. But I've never really been trying to scale anything. And I'm not a super systematic person. And so I'm not like instinctively trying to systematize things. So much more of my activity is kind of going out and being like, "Oh, let me try this product for this thing and see what happens, you know, if I go here and do that and, you know, what's the limits of how much medical history and AI can handle before it, you know, can't absorb that anymore. Spoiler by the way, on that one. They're very, very good." So, I haven't done this kind of like build my own, you know, highly bespoke personal AI infrastructure, for lack of a better term, you know, relative to kind of going out and scattershot doing a ton of stuff, which certainly has the effect of teaching me about AI and very often like does improve my productivity. How do you think the personal AI infrastructure like sets you up for a different lived experience? And maybe give us like some of the highlights to inspire and then we'll dig into how it works. Yeah, I would say the big difference is kind of the main concept that also underlies cloud code itself, which is this whole scaffolding more important than the model, right? So the difference is when your AI understands what you're trying to do. So when you make a request to to a tool, especially, you know, a year or two ago like chat GBT or whatever, it would largely be just taking it out of context. it would just be finding the best answer according to like the model's knowledge. The magic is when it's actually encompassing everything about you and incorporating that into the pursuit of the best answer. Right? So the more your system knows about you, the more it can customize its responses. And it's not trivial customizations. It it's things oriented around your goals. So, I mean, my my challenge to you and to others is to basically sit down and dump via dictation or writing or whatever you want to do or just drag a bunch of documents and be like, "Look, this is you're basically doing a TLOS assessment of yourself to figure out what you think the problems are. Your own problems inside what you're trying to do with your career, what you see wrong with the world or whatever. You sort of dump that. Then you say, "Here's what my capabilities are." You're basically doing this interview with the AI and that builds out the TLO structure of what you're trying to accomplish that is then part of your PI your personal AI infrastructure. Now having that when I initiate cloud code which is running PI it reads my entire thing on startup. So it now knows me. It knows my digital assistance personality and most importantly it loads all my skills which are customized also for me. my blogging skill, my writing skill. I I'm reading this amazing book right now by Mark Foresight. I think it's elements of eloquence, I think, but it's about the uh rhetorical figures going back to like Greek and Roman and basically how to write well. So that's now so basically when I learned that I read this book, I literally have an upgrade skill inside of Pi. I can take any YouTube video, just paste in the link, it goes and gets the transcript. This thing is absolutely insane. It goes and gets the transcript. It reads my entire TLOS, what I'm trying to accomplish. It looks at my full PI system and gives me recommendations on how to upgrade itself. So that means all the skills, all the hook system, all the context, the memory system. So another thing that the PI system has, most other systems don't have, is a system of memory, which is writing signals that I'm giving the AI about how it's doing. And this is like this rotating loop which goes back into the upgrade skill. So it's like okay, how good are we doing as an overall system in helping Daniel to accomplish his goals? How happy is he with the system? And then that just goes round and round to making little tweaks and updates to the system itself. So when Claude Code releases a version, which they did yesterday, I'm looking at it right now. It's 2.1.6. Okay. They released a bunch of capabilities in there that's in their change log. Well, they also might talk about that in an engineering post. They also might have more detail inside of GitHub. I just say perform upgrades. It goes and hits podcasts. It goes and hits YouTube channels to see if anything new came out. It reads every anthropic engineering blog. It looks at the change notes for cloud code. And then it comes back with a prioritized recommendation list of how to upgrade our PI system so that it will work better using the new features. So it's this continuous loop of getting better at accomplishing what I'm doing. I I would say that's the biggest thing. And and just as a little bit of like partial testimony here. I do a lot of bug bounty stuff. So basically finding legal programs where you can find vulnerabilities and get paid for them. And I've got a whole bunch of friends who are in this space as well. and um they're constantly looking for vulnerabilities. I got this one friend. He's a amazing guy. He's a he's a cardiologist. He's over here hacking at the same time he's do he's actually in the clinic and he's he's working with uh patients and everything. He specializes in client side vulnerabilities. So he had been using cloud code cuz I got him onto cloud code. But when he switched to PI, it basically enrolled all of his personal techniques as skills. So now when his Pi loads up, it's thoroughly trained on how he likes to find vulnerabilities, all his personal techniques. So now he could just bring in a target. It goes and gathers the stuff and the number of bugs that he has found has gone massively up and they're paying out more. And pretty much everyone that I've talked to who's using the PI system on top of cloud code, they're getting just much more value. And to be clear, this is the same direction that cloud code is going, right? they're going to have this type of pie like stuff before too long as well. But the the short answer is when it's more when your AI stack, your agentic stack or whatever the term is is more tied to your actual goals and knows more about you, it is just infinitely more capable. Plus, we've got a lot of quality of life stuff. So, I do everything inside the terminal. I'm a Vim person. So, tab completions. I've got a full voice system that uses 11 Labs for customized voices. When I spin up custom agents, they all have their own voices and personalities. So, it really feels more like I'm dealing with my friend Kai than I'm talking to a coding agent that's producing code. &gt;&gt; You mentioned like cloud code is going this direction as well. Can you give a little bit more detail on like where cloud code ends and where pi begins? One of my funny refrains is like everything is isomeorphic to everything else. By which I mean you can kind of always like hide the intelligence and like I find that there's a lot of different ways to structure these things and I'll maybe pitch you on a different one in a second get your reaction to it. But you know there's important functions that you're talking about there where like and I do want to get a little more detail on those too but like context management you know having really good starting system prompts those are obviously key toward like consistently customizing the AI's behavior toward what you want. Cloud code can do a lot of that like where is the line how is the line moving you know what do you think are the most important things that you are bringing to cloud code that it itself doesn't have yet &gt;&gt; so what cloud code doesn't have right now is it doesn't start by saying who are you and what are you about it it doesn't encourage you to bring over your work and your personal goals and your main workflows that you perform in life and uh for your career it's not on boarding you to have clog code be your assistant Okay, it's still its primary identity which started as a coding agent, right? And that's still what it does the best. It's the best at it because they just have the best approach to this. But what I'm building towards um I have I have this thing called uh what is it called? P AIMM uh personal AI maturity model. And it goes from chat bots at three levels, agents at three levels, and then assistance at three levels. And I think right now we're at like agents level two. And when you start getting into assistance, the the world is completely different. So like I'm sitting in front of these screens right now. What should be happening is my AI system should be able to control any of this tech. It should see all these screens. It should hear everything that's happening and I should just be interacting with it. Um one one thing I love to do, I I stole this idea at least partially when I was at Apple. They kind of stole the idea from Amazon, but it's start in the future that you want and work backwards. It's called a PR in Amazon and Apple terminology. So, what we're actually looking for is like her and tars. So, you start with what you actually want, which is an AI that can see and hear and interact with anything you are interacting with. When you say, "Play the perfect song for this moment." First of all, you shouldn't have to say that. You should just play it. But when you say that it should be able to um I got this idea riding in Coyote Hills with my friend Mark on mountain bikes. Wouldn't it be cool because we both grew up very close to these mountains for it to play the perfect song. Well, how is it going to know what the perfect song is? It has to know who Mark is, his relationship to you. What was happening in the 80s when we grew up? What were the perfect songs? And how does that associate with mountain biking in in the wilderness? All of that is context. That's why the scaffolding is so important is because the context engineering is what makes the AI powerful. It's it's not the models themselves. So PI starts with this concept of what are you trying to do. It starts with deep personalization. Your AI has a particular voice. It interacts with you in a certain way. It knows what your capabilities are. It has full access to all your skills. So it's more like you're interacting with a DA, a digital assistant, as opposed to interacting with an AI model that has capabilities. That distinction seems small, but it's actually massive. It it's absolutely massive. &gt;&gt; So it's sort of about, if I try to echo that back to you in different terms, it's really about putting you the person at the center in a persistent way as opposed to with like cloud code off the shelf, we kind of have a project level. Uh &gt;&gt; that's right. &gt;&gt; Focus. And then of course you know we go to the chat itself we have like a task or you know a conversation level focus. In practical terms like how big is your default prompt? You know how much detail is pi loading up or I guess your personal one is Kai and the pi is sort of the empty one that you publish for for other people to to customize to their own individual circumstances. When you're doing your own thing, how much starting information is it getting on every session in it? &gt;&gt; I haven't counted recently. I I want to say probably I think it's something like 10,000 tokens, something like that. I try to keep it fairly clean and it's also responsive. So inside the skill.md file, which is the cloud code structure, I have a whole bunch of other sections which point to specific additional context information. So the skill.md file is like the core. It explains the entire pi concept. It explains where all the resources are. And when uh because that loads initially, I force the load through the startup hook. It then knows how to find all that other information. So for example, I can email people, I can text them, I can do whatever. It knows if I say email Jason or Sasha, it knows who that actually is. So it it can send to the right person at the right time, but it doesn't need to go and read all of those files all at once. This is kind of the advantage of the cloud code skill system is there's kind of three levels. There's the uh front matter which loads by default which is kind of like a routing table. There's the skill.md file itself but then there's references to other parts of the system. So inside of that system I have user system and work and work is like customer like not so much customer but like offerings related stuff. User is very personal stuff and then system is uh the stuff that goes into the PI project. So I mean we're talking about probably like 30 different context files plus the main context file being the skill.md. So yeah it ranges between you know five and probably 15,000 tokens. I mean it's not all that much. There's a lot more context available for it to go get if it needs it. &gt;&gt; You mentioned obviously building this on cloud code but there is open code out there and this last time is is getting so weird but I I don't know. I think it's been like the last 72 hours right now as of when we're talking that Anthropic has changed their policy to not allow subscribers to Claude to bring their inference budget to other projects like open code. Right? So now if you want to use a cloud code thing with at least without paying the API token rate which I understand is easily an order of magnitude more then you have cloud code with cloud integrated and that's like going to give you a much larger inference budget for your 200 whatever 100 or 200 bucks a month versus if you said okay I'll use the API key and go use open code with cloud that now doesn't look like such a great option just because it's going to cost you a lot more and like what exactly are you gaining but obviously with open code you can use a lot of other models and open AAI has kind of you know tried to counter by saying they're, you know, committed to continuing to support these open source frameworks. And it's been funny how Enthropic has kind of followed OpenAI. These two companies, they're very interesting sort of circling each other in so many ways. Anthropic has followed OpenAI in so many ways. OpenAI has followed Anthropic in so many ways. Which one is going to bend on this so they can come back and have ultimately the same policy in the end will be interesting to see. But the question is if I'm as I am, you know, kind of thinking about making a real investment in this sort of thing right now, how would you decide between claude code versus open code and what could you tell me to do so that I can at least minimize my lock in? Cuz I do think I I probably want to go clawed because I like claude. Certainly for like all this personal stuff, you know, it seems like it might be the way to go. But then, you know, I do worry about this sort of lock in and returns to scale, you know, running away with the whole thing. And I I do want to have some sort of off-ramp. So, how do I decide and how do I make sure that I retain as much flexibility as I can? &gt;&gt; Fantastic question. So, the whole agnostic system is is kind of built in from from scratch from PI. It's hard to be fully agnostic because in my opinion, cloud code is just way like generations ahead right now, which could change in the matter of days or weeks or months or whatever, but I I they're so far ahead. So the system is definitely built on cloud code. However, the entire system is markdown files, right? So I'll give you an example. This is a great example of like this whole thing. When open code came out, I switched to it for about 2 weeks. I did a whole YouTube video about it comparing the two. I got great results from Open Code. This was at the moment that Boris supposedly had taken a job somewhere. And this really gets to the answer to your question. If Boris takes a job somewhere or I hear a signal or let's say the cloud code team like 70% of them leave and they all go to the Gemini team or something, I'm going to be switching. I'm going to be switching because it is that leadership. It is the vision that keeps me on cloud code, right? The PI platform is Markdown files. It's skills. It's MCPs. It's context files, right? So that is extremely portable. I could take the PI infrastructure and put it on open code and it would be awesome. it would be much better than most other things just because of the context. The reason cloud code is the base is because there is no other company that gets the concept of a harness as much as anthropic. It's not even close. Google is extraordinary at backend, right? We've we've known this. They are not good at making interfaces. They are not good at empathy. They are not good at understanding what actual human users need and what the interface needs to look like. OpenAI in my opinion is a little bit all over the place right now. I don't see them being as focused on this whole core mission as uh Anthropic is. And a thing that I kind of realized about this which I thought was kind of interesting, it's in the name. Everything Anthropic is doing, it's literally, you know, Anthropic and their art, their messaging. The fact that they're they're constantly warning all the way from the CEO, hey, this is coming. We're worried about you. Please upskill. Please get ready. This messaging of human first has been consistent through the entire thing. And what do you know? They happen to be putting out a product that puts the human first and the human experience first. So this is why I am like 4,000% in the anthropic cloud code ecosystem because the leadership and the vision is there for building this system that PI is essentially. And I just don't see it from anywhere else. And the way that manifests is they're shipping every day. They had like a day and a half of rest over the holidays or whatever and the whole world was like, "What are you doing? When's a new release coming out?" They're like, "Can I take a nap?" It was like insane. But they are shipping so fast. They listen to users. They're live on X, like responding to people. You know, you could ping them and they'll just respond. And it's just like there's no comparison in terms of like having a vision and executing on it compared to the other platforms in my opinion. &gt;&gt; That that's a really interesting take. If I just try to contrast it with OpenAI, it seems like they have a somewhat similar vision in the sense that they sort of want to be your durable personal, you know, they've invested in memory, for example, right? Where you're supposed to feel like the AI knows you from one chat to another. They also now have the Pulse product which kind of at least, you know, suggests a sort of more proactive future. And I do think that product is pretty good. Certainly like most days when I see my pulse notification, there's like something in there that I, you know, feel compelled to click through and check out. I guess one obvious point of differentiation would be just how portable it is. You know, if I have all my like memories sort of locked away in some OpenAI memory store, possibly as explicit text, possibly in some other form that's like hard to do anything with. I think they're trying to create lock in, right, with that product form factor. like they want you to come to Chad GPT all the time because you feel like CHGPT knows you best and can support you best. Is there more to it than portability that you think differentiates those two approaches? &gt;&gt; Yeah, great idea here. I've never thought to like try to separate these two. So, I see them as extremely different, but like you were saying before about how everything sort of rhymes, they're all kind of going the same place. I I wrote this uh really crappy book in 2016 where I was like, look, the future of this is basically you have AI assistants that have all your context. There will be APIs for everything and you'll just talk to your assistant and it will use all these services and like I'm really happy I actually wrote that down, forced myself to get it out there. But I feel like Sam Alman particularly really really gets this. This is one of his big bets and that's the whole Johnny IV thing. And there was a leak that supposedly it's an ear thing. I don't know if you saw that. He is absolutely all in on personal assistant, you know, digital assistant. It knows everything about you. I think he's trying to like skip the whole mobile phone thing and just like this is your platform. And if you look at that personal AI maturity model thing, that's where I'm going as well with PI. In my opinion, that's where Claude Code will end up. Google will end up like everyone's going the same place. It'll be so obvious that it's boring once everyone gets there. It's like, well, obviously obviously everyone's going to build that. Here's the distinction, though. I think Sam is trying to build the device and the interface first in a sort of consumer like disrupt the industry leapfrog over mobile sort of thing. I think that's the direction he's going. Claude code and anthropic they accidentally got here on a different path. And my whole thing with pi is like that that's been like this human first thing which is like on a third rail. So it's like there's the human side, there's the coding agent that gets you there, and then there's the Sam Sam Alman way that gets you there as well, which is like consumer hardware bypass the mobile interface sort of way. But in my mind, like you know, x number of years, I I think honestly like 3 years or something, this this is what the whole space is going to look like is we are reinventing how we interact with technology. You talk to your digital assistant and your digital assistant does stuff for you. the details are all abstracted and that that's kind of already happening with cloud code. &gt;&gt; So when it comes to using something like PI today and investing in this now what is the value driver of that you know for people who aren't like professionally responsible for keeping up with AI. I feel like I have to do it for that reason if no other and at this point I am kind of like I think it might actually move the needle for me. It seems like it maybe is mostly just about training yourself to think and work in this way. like if you if you sort of skipped it, you could in 2728 have probably it sounds like you expect similarly capable infrastructure spun up for you, you know, very quickly by at least a couple different companies that would be eager to be your digital assistant of choice. And so like what do you gain between today and when that is like a really polished consumer product? Am I right to say it's maybe most about like your own habits of mind, your own like strength as a user of these systems or are there other things that you think will kind of help people acrue advantage relative to those that just kind of kick back and and wait for you know the very polished version to become available? &gt;&gt; Yeah, I mean I think the very polished versions will take a lot of time and it'll be highly sort of vendor locked. So for example an open AAI version I'm not sure you're able to see your files and edit. I guess you probably could, but it's going to be a lot more opaque. An Apple version of this, which we're probably going to see this year, it sounds like through Gemini, right? Through Google. So that whole ecosystem of all your Apple data that's now going to be available via if they're going to keep this s name. I don't want to trigger my thing, but I don't know if they're going to keep that name, but like this is going to happen in their world, too. You're definitely not going to have the same access to the environment that you do in in like a cloud code. So, here's my aggressive way of answering your question. Right now is like the craziest moment of like punctuated equilibrium of like the world is changing like so rapidly right now. though you do not want to wait to have an AI platform that understands you and can help you go from I I've got this concept within PI which I'm trying to convert to being the primary center of the algorithm or or the center of the platform but it's a little bit it's outside of my uh working memory and IQ capabilities. So I'm like really trying to push on this thing, but it's essentially uh this thing I wrote about a long time ago, which is that the universal algorithm is going from current state to ideal state. That's the universal algorithm. And and this is within PI. And then inside of that current state to desired state, you have the scientific method. So if you look at like the Ralph loop, have you seen that? The Ralph loop for Yeah. So I've been thinking about this forever. It's like what is the loop that your AI platform is constantly trying to perform on your behalf. So, it's literally saying Daniel is in this state career-wise, personal-wise, and everything. We're trying to get him to this state. And also, when he asks a random tactical question, what is the current state? What is the ideal state? And how do we rotate through this loop to get him there? That is so powerful. You want to start right now with it. you you want to get into a system that can do this for you. I've been hearing really good things about open code lately that they are are actually shipping features and stuff like that. Somebody wants to use open code, I say go for it. I just think most of the innovation on the scaffolding is stronger on cloud code. But I would say do not wait. Do not wait to build an AI that has your tilos and knows what your ideal state is. Because think of it this way. Every time you ask a rando AI a question and get back an answer, the whole purpose of getting back that answer is to do something that furthers your goals. If that is 50% better or 5% better or 2% better inside of this personalized system than it is in a disjointed system, those are true. Those add up. That means I'm going to be way further ahead. Anybody using a PI system in my opinion is going to be way further ahead in in a week or six months or two years than somebody who's using the disjointed uh system. So I would say the worst possible time to wait and see is right now. &gt;&gt; So one other way I can imagine trying to construct something like this and I can definitely see advantages and disadvantages but I kind of want to get your thoughts on them is like so I'm using you know all the frontier companies just mainline products JGBT cla Gemini. I also have been a big fan of Tasklet recently, which has been the sponsor of the podcast, but I genuinely really like using it. It allows you to create these sort of longunning agents that basically have a job for you. And I shared the outline of questions that Tasklet created for this conversation, and I gave it access to drive and, you know, sort of &gt;&gt; It was really good, by the way. It was really good, &gt;&gt; strong. So, I've been pretty impressed with that, but it it doesn't quite have this thing that you're talking about with like the person at the very center of it. It's a little bit less ambitious in scope where it wants to kind of have one job and then you know try to do that job as well as possible and it can take advantage of a lot of context cuz one of the reasons it did so well on this question outline writing process is I gave it access to a bunch of previous outlines of questions that I had done. So it kind of knew what I was looking for the kinds of questions I would generally want to ask. But yeah, it's not like me as a sort of sovereign individual is kind of right at the center of that. But I wonder if there's a way to think about because I guess just one more one more sort of bit on the teaup of this as I've been getting into this a little bit just in recent days. I do find that like oh god you know there's a lot of initial friction right so just for example cloud code okay cloud within cloud code like how can you tie into my Gmail my calendar my Google docs that is like not nearly as easy as one might think it would be or like the choice is not nearly as obvious you know it's like well there's this MCP by this guy and you know there's a few command line tools over here but Gmail doesn't really have a command line tool and if you want to go that route you got to go set up a Google cloud account and have a you know a developer relationship with Google and set up that and then you can do ooth in and I was like okay what is this what everybody's doing in contrast the same company that makes task also makes another email client called shortwave and these things are like short shortwave in particular is like highly specialized and like you know they put a lot of effort into making it a very good way to access everything that I have in Gmail. So, I'm kind of like, hm, if I'm sitting here trying to create my personal AI infrastructure, how much time do I want to be spending on tools and MCPs and skills and developing all of those and figuring out like whether yours is the best or my buddy Chris, you know, does a ton, he's a madman with this kind of stuff, too. And I plan to do a full episode with him and he's got, you know, kind of his version of this. And I think you guys have very different, interestingly, quite different intuitions in terms of like he's very much an open code guy. Both are doing like amazing things, but like which one is right for me? And then I sort of think, well, maybe what I could do or should do is like go use a product like a shortwave where they've done the hardcore engineering of like they even take all your emails and put them in their own vector database, you know, so they can do their own kind of search against your Gmail that's like over and above what Gmail itself allows with API searches and whatnot. And then maybe that model would be like that that thing could sort of call into the sort of you know Nathanbot or the you know the Nathan Tlos oracle that could say you know so when Tasklet's trying to write an outline of questions or when Shortwave is trying to write a draft response to an email maybe those systems are like better specializing in all of the nitty-gritty of the tools and the implementation maybe they call into me and say hey you know here's the context like how do you think Nathan would want to respond to this or you know has there been any goal changes that would change how we would go about writing this outline of questions. Are there any new themes, you know, that are kind of top of mind that that we might want to bring in? So, this is a kind of why I said that everything's isomeorphic to everything else. Like you can sort of see either way working, but what do you think? Obviously, you're betting on this pi framework as opposed to like these sort of products that kind of exist in like a constellation where they're kind of out there off, you know, orbiting around this center thing. I don't know. What do you make of all that? &gt;&gt; Yeah, not not quite. So, you mentioned in the tasklet, you know, why not other models? So this this is a thing perhaps I missed with the explanation here. My I I have a research skill and I have three levels of the research skill. So I if I say do deep research or heavy research or whatever it it spawns all five of my research agents but it spawns eight of them and all of them have separate subtasks. So they all go off and do their work. But guess what? It's not a bunch of anthropic agents. That's Gemini doing that. That's Codeex doing deep research. Those are command line tools. All of my tooling that I actually use, Kai has access to if they have an API, if they have an easy way for me to interact with it. So my personal productivity software that I do, I use to run my team. Kai speaks that language. Kai went and reverse engineered all the MCPs, turned them into Typescript. So I don't actually have to load up any MCPS, which take up a lot of context. But Kai now speaks this productivity software. Kai speaks Salesforce. Kai Speaks email. So I get to bring the best of the best tools to Kai and say this is what we use for this. And what's what's cool about this is that it's exactly what you said. It's best in breed. You don't have to reinvent things. Like I'm not trying to rewrite SMTP. I'm using existing ways to send emails. Productivity software. I'm not going to make a new piece of productivity software. But if I want to replace a piece of software, I could say, "Hey, I don't like paying for this subscription anymore. Go make a piece of software." and it will use all my context, all my text stack, all my design preferences, all my UI preferences and art preferences and everything and it will build that software. So, it's a mixing and I'm also we're going to be adding Olama as well. So, you could use local models in addition. So, when you're using PI fundamentally, it's anthropic, but I've got probably six different model providers that Kai is using because they're better at different things. For example, Google is the best at extremely large uh context and like hack performance. &gt;&gt; But what about this kind of other like does in practice is your number of third-party SAS products used trending up or down? Because I feel like mine is still trending up and I think it sounds like yours is kind of trending down. &gt;&gt; That's an interesting question. I would say maybe down, but I'm definitely experimenting with new things all the time. Oh, and the other thing is uh in my workflow, if I triple tap the back of my phone, it opens uh Open AI because it's the best of breed. Inside my car, I could talk to Grock and Grock is getting extraordinarily good and like the conversational flow, the voice, it's just amazing. I could use OpenAI inside the car, but I prefer to use Grock inside of the car because the user interface. So, I am also sampling all these different tools. I I don't see PI as a competitor at all with any of these because the PI project is just unification around self. And one one other thing I would say it's not so much that PI is putting you at the center. It's more like it's putting your goals at the center, right? It's it's it understands what you're trying to accomplish and it keeps that locked on for its ability to sort of help you do things. But um I'm still other than aic platforms I'm not really messing with right now because I'm on cloud code. But in terms of like model capabilities, specific sort of niche products, I will either use them natively or I will have Kai learn how to use them and then that'll just be part of the ecosystem. &gt;&gt; Do you have any thoughts for like how folks who are making these products like Tesla shortwave, obviously, you know, tons and tons of others should think about the world that you're envisioning. Like I still am kind of wondering if the right way, you know, for me, even as I set all this stuff up, right, and get my goals well instantiated and, you know, build up all the context, should I be like going to that terminal and saying like, go triage my inbox and, you know, tell me what I need to respond to and have the responses drafted that way or should I do it in a product that was really built for email and have that product kind of call into the Nathan Oracle for, you know, what context or judgment assistance it needs. needs at any given moment in time cuz I do feel like a lot of people I mean you're you know you're a seasoned vet when you know a Vim guy as you as you said right like that's obviously a very minority profile and I'm kind of like you know comfortable enough to go do command line stuff but I would probably side more with like the typical user who like wants a a graphical interface or at least is more comfortable with it most of the time. Yeah, that's why I have this maturity model thing to keep reminding myself what the actual goal is and to work backwards. So I should not be on the terminal at all in my PI system and I should not be in some I use superhum by the way that that's my email client but I shouldn't be over there. What should happen is I say what should I be looking at? Who should I respond to? Is there anything important? I just speak those words and the the things happen. Whether in the short term it pops up that client and I have to go interact with it there or Kai is able to do it himself because he can control the client. I think Gemini is definitely getting there very fast with like turning on a bunch of Gemini features in in Gmail. But to me, no, we should not be dealing with any of this cluge of like even an email client is kind of cluge if you think about it compared to like Minority Report or the movie Her. you know, you remember when you onboarded in that operating system. You just say, "Hey, what's going on? Anything I should know about?" And she's like, "Well, I just read your 940,000 emails, you got a new one from Sarah this morning." That's the interface ultimately I think everyone is building towards. So, I I try to keep that in mind. I will say one other thing about you because you're asking about like product advice. The ultimate product advice that I'm seeing and I and I help companies with this all the time especially in cyber security. I have this one piece of advice which is if you are doing a cool product feature in a space like vulnerability management or threat intel or whatever and it's pretty good and you are competing against someone who is also pretty good but they understand the customer and you don't. For example, this is a vulnerability management is a great example for this. Do you know all the engineering teams? Do you know how they push code? Do you know what their repositories are? Do you know how they're measured? Do you know all of those things and their ticketing system and their CI/CD pipelines? If you know that and your vone management program or your vone management solution is a little bit worse maybe than someone else who doesn't have all that context, you are going to lose. Like you're going to lose to the company who has more context. So my expectation is that even somebody who seems like oh we just make a task lit and it just puts out this little piece of context their entire drive they will either not survive or they will move towards the model of you know what it turns out we actually have to learn a lot about this person. We should kind of have like a pie for them and there everyone's going to build this deep knowledge of the customer or the user and that is going to be what powers how good of outputs they can produce regardless of the product. It's another example of what you were saying where everyone's kind of going the same place. &gt;&gt; What's working in memory? I've been fascinated with memory systems for LLMs, AI agents, whatever you want to call them for a while. And this is another area where I feel like everybody recognizes that there's something missing or that you know could be better, but instincts are very different in terms of how to deal with that. So, what have you tried? What is working? Are you using any like dedicated memory infrastructure companies to support your memory features? What do we need to know about memory? &gt;&gt; Yeah, I'm very much team uh file system. Yeah, when the first version of Pi came out like sometime middle of last year, I came down firmly on the on the side of file system. Uh file system is my memory. It is my storage. It is my context management system. I do have an archive of like all my writing back to like 1999 that is like tens of like over 10,000 posts. That one is a rag. So occasionally I have a rag, but I really dislike rag because I feel like it's just lossy and messed up. I prefer file system. I think it's the absolute best. So I have underneath the dotclaw directory in all caps is memory and under memory I have learning, I have signals, I have all these different things that are pulling from the projects directory, the events uh JSONL uh file which is every single transcript that's happening inside of the cloud code system. But on top of that, what I have is built on is this um thing that relates to the algorithm I was talking about. So it is constantly through the hook system determining how happy I am with responses and then the post hook is looking at what the current sentiment level is. I have uh histograms of like how happy I have been with the results coming from the PI system. And so what what that means is the system is designed to look at those signals, look at what I asked for and look at what it produced and then the sentiment and say, "Oh, well, he obviously wants to go more in this direction or he wants to go more in that direction. I should do more of this and less of this." And this is all in service of the ratcheting up of the improvement of this overall algorithm. the overall ability for an agentic system to take for any particular task or for a long-term goal the ability to move from current state to desired state. So, I'm using the memory system to gather extremely granular stuff and all signals, but it's all the entire purpose is self-improvement, recursive self-improvement. &gt;&gt; And does that practically operate on just kind of a runtime agentic search basis where cloud just kind of decides what it wants to look into and pull stuff into context on its own or are you doing some sort of post background batch processing? Because I've I've also been quite interested at times in like one episode I did it was on a system called Hippoor which was taking inspiration from the hippocampus kind of multi-step process where you'd have these whatever your corpus was. First you would go through and do entity recognition and dduplication and then create like a graph structure that would have the entities and then the documents in which they appeared. And that way you could sort of rag into it anywhere in natural language but then see oh that that connects to these concepts which connects to these other documents and kind of you know expand out in a sort of you know network-based way through the corpus as opposed to a sort of purely hierarchical approach to retrieving information. That gets pretty complicated obviously pretty quick. But it does feel like something like that might be needed relative to I mean maybe this is also just my like lack of confidence in my own ability to organize myself and my thoughts well enough. I certainly do recognize people who are quite different in this regard. But it see I feel like I need a sort of crossboundary layer that would probably have to be batch processed in the background to kind of make these connections between all these various disparate things as opposed to being able to like put each one in its proper place such that like Claude, you know, intuitively and correctly uh decides where to go just based on structure. &gt;&gt; Yeah. Well, this to me is the whole advantage of like the scaffolding and like being able to infinitely sort of tweak the scaffolding according to first principles. So, because I have the core skill which is kind of the bootstrap for the entire PI system because I have that laid out and it gets loaded and it has all the context of what like what we're trying to do and everything. It gets also the architecture of the system including the memory system. Now, all this stuff that you're talking about doing with like scripts and stuff like that, that is the clog code hook system. The clog code hook system is extraordinary. So I have I think right now 12 hooks that are active. I've got a whole bunch for user prompt submit. So there's security checks in there. There's sentiment analysis checks in there. It's actually routing throughout the PI system according to what I'm trying to do based on this sentiment analysis which is uses Haiku. So I have a custom inference tool which has three levels of inference fast, standard and smart which is haiku, sonnets and opus. And so the entire system is using this to like sort of self- route. Now the memory system and all those sentiment analyses and all the artifacts of keep in mind it's fully archiving. Cloud code does this naturally. Every prompt I send every tool use that it runs every output of the tool use this is all recorded. It's all there raw for us to an analyze. So I am taking that and putting it inside of this memory structure and I'm overlaying on top of it sentiment analysis. This is all being done dynamically. I'm not seeing anything. It's all just handled automatically due to hooks. So hooks are constantly adding the sentiment layer of how good the algorithm is doing, how good the PI system is doing overall. So at any point in time I could say what upgrades have we made to the system, how have they gone, how has our performance been going in the last month and PI will come back. Kai in my case will come back and say yeah it seems like you know we tried this that didn't work we uninstalled that we went back we went in another direction and currently we're doing this and you seem much happier with this so this seems like a direction to go do you want to do any more work on that &gt;&gt; and that's all just operating on raw logs there's not like a summarization level or some sort of because that sounds like just like a ton of content for it to wade through &gt;&gt; there's tons of summarization happening yeah that's what the inference piece is the memory system is dropping its own artifacts which are summarized versions and they are also creating indexes in JSON L which can be read like instantly fast. So no, you couldn't go and parse like the entire thing all the time. That would be too intensive. Yeah, this is sort of stealing from a Stanford idea called reflections where you get a whole bunch of context and you summarize it like maybe in one line or one paragraph. &gt;&gt; This is from like the AI village uh originally, right? Yeah, &gt;&gt; that's right. &gt;&gt; Yeah, I think about that a lot as well. I got a lot of inspiration from that. So summarizations into indexes which can be parsed and of course they could always go look at the raw log if they want to but they should be able to go off of the index and then that's all happening just with hooks and hooks are happening anytime the system runs. &gt;&gt; In practice when you see people take your system and modify it. How much are they modifying it? Are people like following in your footsteps, you know, relatively closely or h are they kind of veering off in in all sorts of different directions? &gt;&gt; Yeah, I've not seen many modifications. It's more so population of the system. Someone just posted one yesterday to the discussion in on GitHub. Holy crap. I was like scrolling. It was like 20 pages. It was like the most insane thing I've seen. Oh, I think the guy's name is Jim and maybe the agent's name is James. I can't remember. Something like that. But anyway, he just brought over so much context and so many things and it it was just massively impressive. So it's just a matter of he knew exactly what he wanted. This is what activates pi. He knew exactly what he wanted. He's been struggling with all these same pi problems of you know pi not existing, clawed code not existing in the past. He's been sitting on all these things like I have for decades. He knew what he wanted. He knew what he wished he could do. He saw a pie, brought all the stuff over, and now he's producing way more content. He can make products. So, it's more so like activation of what was already there, but kind of dormant. I have seen some expansions of the system. There's lots of feedback, pull requests and stuff where they're like, "Hey, could you add this? Could you tweak this or whatever?" And so, we're obviously trying to listen to those. &gt;&gt; How does it feel to you? This is a bit of a weird question. You know, we have obviously highly plastic brains that can really surprise people in terms of just how adaptable they can be, right? And here I'm thinking like blind people, you know, seeing through a prosthetic that zaps their tongue and they learn to interpret that as a visual signal. You know, the list is kind of long there, right? I think it was sure if I can say his name quite correctly, but Geron Laneir, oh yeah, &gt;&gt; hopefully I'm saying that right. He's done fascinating experiments with like virtual appendages in kind of VR and you know getting your brain to learn to control some like prehensile tail or something like that you know and and you can actually kind of learn to do it. I'm wondering and then of course I'm also thinking like Neuralink right is about to apparently start scaling up its customer base. Obviously their ambitions go way beyond treating paralyzed people and you know who knows what that's going to look like in the future. Is there a feeling that you have of like this thing being a sort of literal extension of you where like if it's turned off, you know, or you don't have access to it for a time, do you like begin to feel like something is missing? Another version of this, real simple one, but digital is like the feeling of something being on your clipboard. I know like this is something I I recently like looked this up. It's a fairly known phenomenon. I've always felt like, you know, for for 20 years now, I felt like I know when something is on my clipboard. I sometimes don't know what it was anymore and I have to like paste it to see what it was but I know that there's like something there that part of my brain has kind of developed or you know changed in some way shape or form to be tracking that very closely and it is a felt sense that there's something on the clipboard. So I wonder how this feels to you and if if you can describe that I'm kind of this is sort of like a way to try to get at what the end state would look like if I'm using this kind of thing. How should it feel to me? you know, well, how will I know that I'm like hitting pay dirt based on feeling how it feels to you right now? &gt;&gt; Yeah. Yeah, totally. I I love that you brought this up. So, I think I was way back in the army in the '9s and I came across this book called Getting Things Done by David Allen. And ever since then, let me reach into the pocket here. I have index cards and index cards are like my way of capture. So the the prime directive for David Allen is never let anything sit in your brain because it will hassle you and trouble you and cause like executive function problems because your brain will be like, "Hey, what about hey, what about hey, did you remember that thing?" So I'm a massive clipboard person. Uh not not technically clipboard, but in the way that you said. So in front of me, I've got different colored sticky notes. I have this system. I have my space pen, which is uh my favorite gift to friends. And like this is just what I travel with to make sure. And now I have this Limitless pendant which just got bought by Meta by the way. So I think I might switch off of that. Capturing what I'm thinking at the moment has been critically important to me for like over 20 years. It it just feels like massively important. I just recently created a reminders file inside of Pi. So I could just say, "Hey, remind me to do this. Remind me to do that." But honestly, the vast majority of that is I have like 2900 Apple Notes. So Apple Notes has been kind of my main capture for a long time unless I'm doodling or capturing ideas like visually which is on the cards. Now again going forward I should not have to be doing any of this. I'm going to keep my cards just for history reasons but what should be happening is more like with her you know walk Phoenix it's like hey make sure I don't forget this. Hey make sure I don't forget this. And Agentic Systems should be switching away from like call-in response to your reminder list is always there, always ready for your DA to shoot you a prompt. Hey, it's time. This would be a good time to do that. Hey, um do you want to uh revisit some of your to-dos? I saw a really cool thing on X uh yesterday. It's like a little clock um next to them and it's the daily agenda in analog form on this digital clock or whatever on their desk. Clock code generated, right? So whatever they're doing, they must have their own PI system and it's right there in physical form. So it's like crossing these two worlds which I really like. &gt;&gt; How do you think about the triggers for the system? Obviously you can ping it, right? And then presumably it can be pinged by any number of external or you know you can allow it to be pinged by any number of external events in the world. And then there's the kind of background processing or you know if you want it to be proactive for you is that like a daily job or an hourly job? What do you think is the right balance between you go to it, it runs on a schedule, something triggers it, you know, from the the rest of the world or maybe some, you know, uh, mysterious fourth thing? What's the right way to to think about that balance? &gt;&gt; Yeah, that that's that's a wonderful question. So, I'm in the process right now of building because cloud code is just now releasing this. Anthropic put out cloud code team. They now have the ability to launch remote agents. So you can actually send a task and it will run off in a GitHub infrastructure in their environment and then return results to you. The other thing I have I'm a big Cloudflare person. So Cloudflare has the ability to create workers that have can run different things on different scheduled time frames. Most of my infrastructure is Cloudflare and they can talk to each other via authentication and access each other. I even have an infrastructure for running cloud code inside of a docker which agents can also talk to and schedule. So all of this is in service of again going back to the what I was talking about before. I should not have to think about any of this. I I do right now because the text's not quite there. But when I want to make something like you're talking about I literally say to Kai, hey look, I need you to not forget these things. I need you to remind me these things on a regular basis or whatever. what are possibilities? And Ka will be like, well, yeah, so listen, right now the whole trigger thing, like that's not super far along. Uh, I tell you what I could do. I could spin up a worker. I could check every 5 minutes or every 1 minute against this set of goals and I could ping you like how would you like me to ping you? Uh, we could do the Discord thing. I could text you. I could send you an email. So, we're starting to like creep towards this in kind of like a cluji type of way, but it's another example of everyone's going the same place, right? because everyone's talking about background agents right now, remote agents versus local ones. Part of the PI maturity model is, and some of my friends are ahead of me on this, they're already calling in and accessing their terminal remotely. Me being a security person, I'm uh scared shitless about this, so I haven't done it yet cuz I haven't found a perfect secure way to do it. It is a huge problem that my system is a terminal inside a computer. if you want to get to the future of her, that's got to be with you all the time, right? So, that's all stuff I'm thinking about. And scheduled tasks, like you said, or logical triggers is even better. It's better than scheduled tasks cuz like one of the first things I talked about in that in that uh book in 2016 is just like proactive. That's a huge difference. Call and response, that's one thing. It's really cool, but it's still too close to a chatbot in my mind, right? You're like, ask a question, get an answer. Cool. Now, you have to do something with it. What should be happening is it understands your environment, the timing. Like right now, Kai should not be interrupting me with like, "Hey, did you see this cool news story?" Because it knows I'm in the middle of a conversation. Small little movements all in these directions from multiple angles, I would say. &gt;&gt; So earlier you mentioned that your friend is like earning more bug bounties by doing something like this. Do you measure your own productivity in any similar way? And you know, how much boost do you think you've got? And then as this presumably continues to create more and more leverage, that seems to imply that like you'll have to have yourself in the loop, you know, with lower and lower frequency, right? If in the limit of this sort of thing, you're only able to like review so many things and make so many decisions. This is, you know, the gradual disempowerment people would be saying, "Hey, you're you're talking about it right now, but you if it's performing well enough, you know, you'll be reviewing the things that matter and you won't be reviewing the things that don't." What can you measure about your own you know output today and where are you in terms of like how much scope of action you give the system like does it ever send a response to an email does it ever send an email as you that you didn't review or do you like allow it to respond as itself you know without you know signing it as you but like still try to move things forward without you actually being in that loop would you allow it to spend money on your behalf without you like you know signing off that like yes you want to execute that transaction. Are there other frontiers of action that you're kind of watching the line move on what you do and don't need to be looped in on? &gt;&gt; I would say that being kind of naturally a little bit cautious. I would say the scaffolding is not there yet for a whole lot of trust in this regard. When I'm sitting here watching it, I've got um a big part of my hook system is actually a whole bunch of defenses watching it, watching what the agents are doing, making sure it's not accessing certain files and directories. and that uses the clog code underlying system. It's got a whole bunch of cool permissions. I don't run dangerously skip permissions anymore. I used to I turn that off. So, I've got a whole like security scaffold there for file system access and stuff like that. Then I have a whole bunch of prompt injection defenses cuz those are massively dangerous as well. And I kind of keep those layered. I just don't feel like the scaffolding is there yet to be like, hey, whatever. Here's my bank accounts. Uh just run with it. I would say I'm okay with experiments. Okay, here's a separate bank account. It's only got $1,000 in it. Like, go crazy. Like, you've probably seen the vending machine benchmark. &gt;&gt; Yeah. &gt;&gt; Yeah. Like, cool. If there's bounds, if there's like blast radius control, sure. But when it comes to like being able to send out emails and you know maybe my diary is sitting um I don't actually have my full uh diary or journal in the system yet because this is one of the things I'm a little sensitive about. But um somebody sends me a link that's like hey uh Kai should go read this. I send Kai to go read it. It's a prompt injection and pretty soon I I just published my diary on LinkedIn. I mean that's possible right? Much harder to do against me but um prompt injection is not like a super solvable thing. So I would say I level of trust I'm going to say I don't know there's no way to put a number on this but I'm going to say like 60%. And I think over the next couple years I'll probably get to 80 90%. But I'm still going to have I still think security also being ex-military and you know just cyber security. I think in terms of threat models here's all the things that would super suck if they happened. Just assume they happened. What could have stopped them? And a lot of that comes down to impact reduction in addition to probability reduction. &gt;&gt; It's fascinating to think that you're not a total maximalist on this stuff. &gt;&gt; I am. I am. I'm a total maximus on it, but it's just I do lots of crazy experiments. I just have the blast radius limited quite a bit. &gt;&gt; Yeah. I'm not a total yoloist, I guess, maybe is the uh Yeah. &gt;&gt; is maybe a better way to say it. &gt;&gt; Well, I've kept you a long time. I could go on longer, but I should probably get us wrapped up and I got to get um I got to get deeper into this is obviously the next big thing for me to do. One other thing I wanted to touch on from your PI principles and then maybe just give you kind of a chance to touch on anything that we didn't touch on that you think I should know or anybody in the audience should know. But the last principle was permission to fail. And I thought that was quite interesting. It certainly brings to mind things like when Anthropic gives Claude the option to end a conversation because it thinks it shouldn't be having this kind of conversation or to like escalate something to the model welfare lead at Enthropic, it brings the bad behaviors of deceptive alignment, etc. down a lot to give it that sort of escape valve. So, it sounds like you're doing something very similar there where you're saying if you can't do this, don't gaslight me. Like, it's okay to fail, but just come back and tell me the truth. I think that's a really interesting fact that people should appreciate better about AI in general. And it's interesting that it's made your list of principles. Interested to hear any more about that that you want to share. And then maybe just anything else that you think people, you know, that I didn't touch on that you think people should not miss out on. &gt;&gt; Yeah, I I'll talk about that real quick. I think that's a very tactical one that we just understand as being a weakness of LMS more so the further back you go. This is a huge problem in 23 where it would just make up stuff because it's trying to do the right thing. So this is a very tactical thing basically saying it's okay if you don't have the right answer it's okay if you can't get to ideal state feel free to tap out and just tell me the truth because I value the truth more than you trying to keep you know confabulating something so it absolutely does it looks like from the studies it does actually improve performance especially in not hallucinating and being psychicopantic and all that sort of stuff as terms of like positive or other things to mention I I would just say that um I've had this idea of a slack in the rope for a very long time. So the idea is I I feel like that as humans, we talked about us not being unlocked. I feel like as a species, we tend to feel like the way history has gone has gone that way because of our innate human limitations. It's like this because that's the only way it can be. We only have these medicines because we're at right we're right at the limit. all of science is pushing perfectly with full strength and this is the exact place and to go 1% more would take infinite energy. I don't think that's true and I think AI more and more is showing you that this is not true and I am so bad at this because I'm also programmed. I'm constantly trying to break myself out of this of like no once we start asking the right questions and providing the right context we're going to be like are you kidding me you are at 1.7%. And it's really easy to go to 63%. And we've seen this with AI models actually for a long time. And I was arguing with, you know, some of my friends at these labs back in 23. They're like, "Yeah, whoever has the compute is going to win." I'm like, "Well, aren't there like little tricks where they're like, "What if we just reverse the numbers and add them this way instead of that way? Oh my god, 47% increase. How many more of those are like lying on the ground? Just fruit ready to eat that. It's just a matter of doing these combinations. How much research out there is partial? The uh medical research, this one trips me out. It's like how many studies did like grad students do, and they're like, "Oh, it turns out this molecule, if it encounters this part of a cell, it will produce this antibbody, and this antibbody um will, by the way, kill all bad things. Hey, listen. I got to go take this job. Um I'll just leave this research paper here." and it's in some file somewhere or physically printed out somewhere and no one's looked at it. But there are hundreds of thousands of these across decades and it's like going back to the security problem. No one has the time or the eyes or the brains or the hands to actually go and look at this stuff. So I feel like the combination of these two concepts means we're nowhere near any limits of what we could do. Like there's just so much opportunity. And when you start looking at things like everyone gets a tutor. Oh, here's here's a crazy one. Here's a crazy one. What if we could not only change what we could pursue based on what we want. So eliminating the obstacles in front of what we want. That's cool. That's what we've been talking about. What if we could change what we want? There's this whole concept in philosophy of like there's what you want and there's what you want to want. So it's very hard to be like, "Yeah, I just really wished I like celery." How are you going to do that? Now a drug comes out GLP-1 or whatever the agonist GLP1 agonist. It literally makes you not want food. Okay. What if I wanted to be more selfdisciplined? What if there was an unlock for making me 10% smarter, which I would love both of those, right? These I feel like we don't know. It's a it's an open question of which ones are easily slack in the rope fixable and which ones actually are, you know, physics that are stopping us. But I think a lot more problems in the world are likely to be the former. &gt;&gt; I think that's probably a great place to end it. An aspirational note. I'm looking forward to digging in on this a lot more and I really appreciate your um walk through today and so many aspects of the positive vision for the future that you've shared. So Daniel Mesler, thank you for being part of the cognitive revolution. &gt;&gt; Thank you so much. I really appreciate it.

## Minhas Anotações

