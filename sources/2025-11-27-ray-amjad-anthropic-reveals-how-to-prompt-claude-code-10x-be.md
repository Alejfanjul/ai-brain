# Anthropic Reveals How to Prompt Claude Code 10x Better

## Fonte
- **Tipo:** video
- **Autor:** Ray Amjad
- **URL:** https://www.youtube.com/watch?v=pb0lVGDiigI&t=10s
- **Duração:** 14 min
- **Data original:** 2025-11-27
- **Data captura:** 2026-01-04

## Conteúdo

Kind: captions Language: en By the end of the video, you will be significantly better at prompting Claude's Opus 4.5 model because I'll be going through many of the best practices that the Enthropic team have shared on their own website and that many of their staff have shared on Twitter as well. I'll be going through a lot of the ideas in a much easier to understand format with diagrams because going through the website can be pretty boring. But before getting started, I should share that I have a Black Friday sale going on right now on my Cloud Code masterass. I go through every single feature of Claude Code in the class as well as a lot of bonus content that I haven't talked about before on the channel and you won't find anywhere else on YouTube. I also have a Black Friday sale going on right now for my own application, Hyper Whisperer. There will be a link to it down below. Anyway, so a lot of the ideas we'll talk about will also apply to Sonic 4.5 and HighQ 4.5. First of all, when prompting the model, you want to use a less aggressive tone because using an aggressive tone by saying something like you must always use a web search tool or something like that can trigger the model into like fearing making a mistake and it will trigger tools that it didn't actually need to trigger and that can lead to a more chaotic output and then basically lead to worse like performance overall. But if you use a more directed prompt by saying something like use tool X when condition Y is met, answer directly otherwise, then it will perform much better because it can just follow procedures that you have given it. And of course now because it's not using tools unnecessarily out of the fear of making a mistake or errors, it will give you better and more precise outputs instead. And I thought this metaphor was pretty good because treating the model like a rebellious teenager by always yelling at it and saying like you must do this, must do that can cause like more anxiety in the model and then basically lead to worse performance compared to just treating it like a competent employee instead. Now over a year ago some research did show that using negative prompts did lead to better performance out of some models, but it seems that time is slowly coming to an end because that can lead to overusing tools in a certain way and just lead to worse output overall, especially when coding. Some bad prompts would be something like, "You must use a search tool whenever the user asks any question. Always search first before responding. Never skip searching." That would lead to worst output and be less efficient than saying something like, "Use the search tool when you need current information or facts that you're uncertain about." And one thing that Anthropic team did make and will be linked down below is a cloud code migration plugin. So basically, by installing this plugin, it changes many of the system prompts that may be in your codebase to be less aggressive. because they say that aggressive language can lead to tool overt triggering and they give some examples here of how you can use softer language. Next up, you want to be more actionorientated when prompting model. So, previously if you gave a earlier version of a cloud model a vague prompt like can you suggest improvements as code then what it would do is it would actually often do an implementation for you instead of actually suggesting improvements. And if you're still using the same prompt hoping that it will do an implementation for you, it no longer will actually do the implementation and it will just give you suggestions instead which is exactly what some people do want because now it follows instructions more precisely. So if you were hoping that it would actually do the improvements for you instead of suggesting them, then you want to be more explicit and actually say like can you refactor this code and implement the changes directly. So previously saying something like suggest would actually lead it to carry out the implementation that you gave it. But now it won't do the implementation because it will just give you suggestions. Another important idea is that Opus 4.5 now has a tendency to overengineer in some ways. For example, if you gave it an open-ended request, which you may have done previously, you would have said something like refactor the authentication logic and then claude opus 4.5 because it's really eager to impress. You may think like, okay, I'm going to come up with a new interface, come up with new helper classes, a new config file, and more like abstraction or something like that. and then it like makes much more changes than you would have actually anticipated. And in reality, you may have actually only wanted one or two of the files changed instead. So instead, what you should do is you should be more precise and constrained in your prompts. So you should say something like refactor authentication logic, modify in place, no new files, keep it minimal. And it's like, okay, understood. Like we will only change any existing files. So this is pretty important if you didn't previously already have a habit of constraining your prompts. You can also add similar logic to your cloud. MD file for cloud code. So you don't always have to write this. Another important point is that Opus 4.5 can be conservative now when exploring your code. So you want to encourage exploration where possible. So for example, if you now say fix a bug in the user module, then instead of looking at the surrounding context and seeing how the files relate to the user module, it will just like make up some like idea of how the rest of the codebase works based on its training data. And that can lead to like a broken fix overall because it's missing important context. It's just like guessing what kind of things the other files would include and then lead to more errors. But now if instead you say read the entire user module and its dependencies, understand the data flow, then propose a fix, then it will actually go ahead and start exploring the codebase more, building a mental model of how everything works and relates together. And then whatever fix it does come up with will be more accurate and based on all the remaining and surrounding files. Of course, if you are using plan mode within cloud code, that will trigger off sub agents that will explore the codebase for you. So you may not have to worry about this quite as much. But if you're not using plan mode, then you should either trigger an explore sub agent or you should get cloud code to do the exploration itself. And remember that you can trigger the explore sub agent manually by doing at and then explore and then just giving your prompt in here. Now if you also want richer outputs from the models, you have to give richer prompts. So if you just say build me a login page in React, then it would give you a really simple bare bones like output like super concise like not a lot of information there. But if you say something like build me a login page in React include forful animations, form validation, error messages, accessibility features, a polished visual design, go beyond the basics, then it will give you significantly better output, which means that Opus 4.5 will deliver comprehensive featurerich solutions when explicitly asked. It will end up being more concise and give you more minimal like outputs if your prompt is quite minimal and concise. I do go through this in a previous video where I teach you how to use Claude's brand new front-end design skill to come up with better designs with cloud code. But essentially what's happening right now with the models is that when you give it really simple minimum prompts, then you're facing something called distributional convergence. So the model has a lot of training data distributed in this way. So you have like unique fonts, bold colors, complex animations, and all this stuff. And all of our uniqueness here is kind of washed out and merged and mixed together. And basically you get this like high probability center instead. So any patterns that are safe, common, like well used like popular fonts and so forth are converged all together into like a interpawn purple gradient minimal animation because these kind of designs are universally acceptable but also very generic as well. And this can be useful when you're coding with cloud code itself because if you ask cloud code to implement a authentication system for you, then you don't want it to come up with a brand new wacky unique authentication system because that may lead to security risks and you want cloud code to write our authentication system that actually works and is universally well used. But of course, now if you're using the same logic that cloud code uses to make a secure authentication system that like works universally, it can lead to worse performance when it comes to design. So essentially what you want to do is offer cloud code explicit guidance such as avoid inter fonts, use unique fonts, add complex animations, create a bold color scheme and then it will like avoid a lot of the ideas kind of in the center over here and then combine related ideas on like the edges and the vicinity of the training data that it has. And that can lead to better output overall as well because it's directed to sample from the like creative and specific regions to come up with better designs than sampling from like the center of the like training data cloud that it has. Of course, this is somewhat of a simplification, but I think the diagrams do help you understand that idea. But yeah, I think a good way to think about this is that when it comes to design, the model will put in more effort into it output if you put in more effort into your input. Next up is that if you're providing the model with rules, then if you provide a very bare like basic rule like never use abbreviations, then the model will be like, man, why is this rule so arbitrary? Like am I just going to follow it blindly? Like what is the motivation here? Then it will basically follow the rule very blindly and say instead of saying like versus like VS, it will say versus and instead of it saying etc for etc it will use like the full like version and that can ultimately lead to like very awkward output that you did not intend for it to give you. But if you are giving a rule then you should give some motivation for why that rule actually exists. So if you say never use abbreviations then add some context such as for formal legal documents to avoid ambiguity and then claude code is like yeah I got it like I understand the goal is clarity and formality I will apply this principle broadly and then basically that will allow the model to understand the why behind the rule and then generalize the principle that you are getting at to create higher quality outputs. Now you're probably thinking what does this mean practically when you're coding? So you could have a rule that said something like always use try catch blocks around API calls. But after if you expand on the rule always use try catch blocks around API calls. Our monitoring system relies on court exceptions to trigger alerts and unhandled rejections cause silent failures in production that are difficult to debug. Or before you would have had a rule saying something like add comments explaining complex logic. Afterwards you would have a rule like add comments explaining complex logic especially business rules. Our domain insurance claims processing has non-obvious requirements that aren't apparent from the code itself and future developers won't have access to the original stakeholders. And what this means is that when you generally write like this, rules generalize intelligently, which means that the cloud models understand the principle behind the rule and not just the rule itself. And that allows it to make trade-off decisions. So for example, if you have two rules that conflict and you didn't realize they conflicted, then it knows what concern matters more for that particular case and then it won't waste time being confused like not knowing which rule to actually apply here. And especially if your cloud MD file like fills up or you have like many different things across the project, then some rules may begin to conflict which is why having the motivation behind the rule written with the rule itself is important. It also allows a model to flag edge cases. So if there's a situation where there may be an exception of sorts because like some library requires it or something like that then it will be able to flag that to you and also it will be able to apply the spirits of the rule which means that opus 4.5 will be able to handle novel situations but that rule may not apply exactly but it comes up with a no good idea because it knows the motivation behind it. So essentially if you're writing a rule you should say the rule statement the business and technical reason behind the rule and then any consequence that may happen if it's violated or like whatever benefit comes from actually having followed the rule. Next up is about verbosity. So by default like the cloud 4.5 models tend towards efficiency and may skip verbal summaries after tool calls and just jump straight to next action. And this can create a more streamlined workflow, but you may prefer more visibility into what's actually happening behind the scenes. So, for example, like if you just leave it on the default verbosity and you say to cloud code because you're using it for something non-coding related because it's a good agent overall, you say something like research competitive pricing and then update the spreadsheet, then cloud code will be as efficient as possible and then give you a final output and you're kind of like hm why did I get this output? Like why is it like this? essentially like the model's process has been hidden after each tool call that it did and each information that it fetched online and then you would probably be more confused about why it gave you that particular output because you can't read any reasoning behind what happened. But if you say something like research competitor pricing and update the spreadsheet after each major step provide a brief summary of what you found and what you did then it's kind of like okay well firstly I'm going to find the competitors found free like here are the websites and then it finds the data extracted this and these are the variations and then it like tells you what exactly it updated and then when you're going through the history you can read through everything that it did and it feels like the entire process is more transparent and you can feel more confident in the solution or like output that it gave you and you can also include something like this. After completing a task that involves a tool use, provide a quick summary of the work that you've done. Now, next up is controlling the formatting of the responses that the cloud models give you. So, you want to avoid negative prompting when it comes to formatting. So, you may have said something like do not use bullet points or markdown formatting and that basically imposes a bunch of restrictions on model and then the model is too focused on what not to do and that can lead to more awkward and unnatural outputs and responses. But if you use more positive framing and are more directive and say something like write your response as flowing pros paragraphs use complete sentences that connects ideas naturally then that will give you a significantly better output than if you tell the model what not to actually do and this can be pretty important when you're using the cloud models quite a lot when it comes to writing. Next up is that the cloud 4.5 models excel at parallel to execution. And Sonet 4.5 is particularly aggressive when it comes to firing off multiple operations in parallel. Which means that if you're using it for research, for example, then it may run three or four web searches in parallel which are more speculative. Whereas if you got it to run it one by one instead, then the next research query may be based on the results of the previous research queries results. So for example, if you get it to read a bunch of configuration files and summarize the differences, then often it will do it all in parallel and then it may lead to a more chaotic output than if you got it to do it one by one or 10 by 10. What this can look like more practically is that when it comes to coding, if you have it run multiple like install commands in parallel, then it may be the case that it installs two things and they are conflicting in some ways and then it gets stuck in like a loop or like a cycle. Unfortunately, this behavior is pretty steerable because if you included this prompt in your cloud MD file, then it will like be even more parallelized and it will do more things in parallel whenever possible. And if you want to reduce the amount of parallelization that it does because you notice the output quality being worse, then you can use this prompt instead to ensure that there are pauses and like faults between each step. And another problem that can happen with the cloud 4.5 models is that you should try avoid using the word think when you have thinking mode off in cloud code for example especially when using opus 4.5 because that's particularly sensitive to word. So basically if you use a word think with a cloud model and you don't have thinking mode on and you say something like think step by step before answering then it can lead to a bunch of noise and like a long-winded answer because it's not actually capable of thinking. So if you need to have thinking mode off then you should say something like carefully consider and evaluate constraints then give a final answer no intermediate reasoning. So saying words like consider, believe, evaluate have a similar meaning and it can lead to better output quality with thinking mode off then using the word think. But yeah, that is basically how you can be prompting Opus 4.5 and the other cloud 4.5 models really well. There are some sample prompts that you can include in your cloud.md file in this page, but it is like very long to read through. uh but may be handy for certain people. And remember that there is a Black Friday sale going on right now for my Claude code masterass and also my hypoer software. Both of them will be linked down below.

## Minhas Anotações

