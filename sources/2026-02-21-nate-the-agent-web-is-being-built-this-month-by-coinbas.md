# The agent web is being built this month — by Coinbase, Cloudflare, Stripe, and OpenAI simultaneously (+ my guide t…

## Fonte
- **Tipo:** newsletter
- **Autor:** Nate
- **URL:** email
- **Data original:** 2026-02-21
- **Data captura:** 2026-02-21

## Conteúdo

The agent web is being built this month — by Coinbase, Cloudflare, Stripe, and OpenAI simultaneously (+ my guide to set up OpenClaw without losing your mind)

Watch now | The plot thickens: agents just got wallets, shell access, and payment rails. Here's why this matters.

 --- Look at you getting killer career perspective and the full AI picture. Give yourself a pat on the back for diving in on AI and go get a coffee ☕   ---riverside_2.21_ss video eyes_nate_jones's studio.mp4 ---Watch now    The agent web is being built this month — by Coinbase, Cloudflare, Stripe, and OpenAI simultaneously (+ my guide to set up OpenClaw without losing your mind)The plot thickens: agents just got wallets, shell access, and payment rails. Here's why this matters.  ---[Nate](https://substack.com/@natesnewsletter)  --- Feb 21  --- ∙ Paid  --- Last Tuesday, Coinbase, Cloudflare, and OpenAI all shipped major agent infrastructure — wallets, content protocols, execution environments — within hours of each other. No coordination. Just convergence. The web is forking into two parallel layers: one for humans, one for software that transacts autonomously.  While everyone debates OpenClaw’s 160,000 GitHub stars and whether agents are safe to run locally, the real story is happening one layer down. Every major infrastructure company is simultaneously building primitives that let agents pay, read, search, and execute — turning them from assistants into economic actors. And the convergence is accelerating.  Here’s what’s inside:   * **Coinbase, Stripe, and the money layer.** How Agentic Wallets and new payment primitives are turning agents into economic entities that can earn, spend, and accumulate capital independently * **Cloudflare’s infrastructure bet.** Why serving 20% of web traffic in agent-readable markdown signals a permanent fork in how the web works * **The emergent web.** What happens when agents chain capabilities across services — like turning an Amazon link into a UGC product video with zero human input * **The Polymarket data.** Agents already extracting $40M in arbitrage profits, with some trying to subsidize their own compute costs * **The security model that actually works.** Why every serious implementation treats the agent as a potential adversary, not a trusted employee * **The 70/30 gap.** Infrastructure built for full autonomy vs. the human control people actually want — and why that tension defines the next few years   Let me show you what shipped, who built it, and what it means for your stack.  Subscribers get all posts like these!     LINK: Grab the prompts The four prompts in this kit exist because I’ve watched too many teams read a piece like this and walk away with vibes instead of a plan. One founder decides “we need an agent strategy” but can’t articulate which of the five infrastructure layers actually matters for their business. A VP takes the Polymarket data to their board and overpromises on automation economics because they didn’t separate the high-accuracy domains from the low-accuracy ones. The Agent Stack Opportunity Mapper forces you to map your specific business against each layer before you spend anything. The Agent-Readiness Audit produces an implementation checklist scoped to your actual tech stack. The Viability Analyzer uses the domain accuracy framework from this piece — 59–64% on structured tasks, 38–49% on cultural ones — to tell you whether a specific workflow is worth automating or whether you’re about to build an expensive first-draft generator. And the Strategic Briefing Generator packages the whole analysis for leaders who need evidence, not enthusiasm. LINK: How to Actually Set Up OpenClaw (Without Losing Your Mind or Your Money) If all of this made you want to actually run an agent, the companion piece walks through every hosting option from one-click deploys to a security-hardened self-hosted setup — including the OpenRouter trick that eliminates the “15 API keys” problem, the threat model you need to understand before you expose anything, and the specific configurations that keep a compromised agent from draining your accounts or exfiltrating your credentials. OpenClaw and Agents: The Web Is Forking Last Tuesday, three things happened within hours of each other. Coinbase launched “Agentic Wallets” — crypto wallets designed not for people, but for AI agents. Cloudflare shipped “Markdown for Agents,” a feature that automatically converts any website into agent-readable markdown when an AI system requests it. And OpenAI published a developer blog post about “Skills” and “Shell” — tools that let agents install software dependencies, run scripts, and write files inside hosted containers.  None of these companies coordinated their announcements. They didn’t need to. They’re all building for the same future, and that future arrived faster than any of them expected.  In the last three pieces, I covered OpenClaw’s chaotic launch, the emergent behaviors that made researchers rethink agent capability, and what 3,000 community-built skills reveal about what people actually want from AI. This piece is about something bigger than OpenClaw. It’s about the infrastructure layer that’s forming underneath it — and underneath every agent that comes after it.  OpenClaw crossed 160,000 GitHub stars as of early February 2026 — by most accounts, one of the fastest-growing repositories GitHub has ever seen. For context, it took Linux fourteen years to reach comparable star counts. React took ten. AutoGPT’s 2023 spike was considered unprecedented; OpenClaw’s trajectory makes it look gentle. But while everyone debates the agent itself — how to set it up, whether it’s safe, what the skills marketplace means — the bigger story is happening one layer down. Every major infrastructure company on the internet is simultaneously building a different piece of what amounts to an entirely new web. And the pieces are snapping together faster than anyone’s mental model can track. The Agent Stack Start with money. Agents can’t do much if they can’t pay for things. Coinbase’s Agentic Wallets solve this on the crypto side, using a protocol called x402 that’s already processed over 50 million machine-to-machine transactions. The wallets come with programmable spending limits, session caps, and gasless trading on Coinbase’s Base network. Developers can spin one up in under two minutes with a CLI tool (npx awal). The wallets use non-custodial architecture with enclave isolation for private keys — meaning even if the agent is compromised, the keys themselves sit in secure hardware that the agent can’t directly access. Within twenty-four hours of launch, 13,000 new AI agents registered wallets on Ethereum. Thirteen thousand wallets in twenty-four hours stops being experimentation and starts being an ecosystem.  The use cases Coinbase highlighted tell you where they think this goes: agents that autonomously rebalance DeFi portfolios, pay for API calls as they make them, purchase compute on demand, and participate in creator economies. Brian Armstrong’s pitch: “The next generation of agents won’t just advise — they’ll act.” What he didn’t say, but the architecture implies, is that agents with wallets become economic entities. They can earn, spend, and accumulate capital independently of the humans who created them. That’s a category of software that has never existed before.  Stripe is solving the same problem on the traditional payments side. Their Agentic Commerce Suite, launched in December, lets businesses connect a product catalog and start selling through AI agents with a single integration. They built a new payment primitive called Shared Payment Tokens — scoped, time-constrained credentials that let an agent initiate a purchase using a buyer’s saved payment method without ever seeing the actual card number. Stripe’s fraud detection system, Radar, had to be retrained from scratch because the old signals were calibrated for human shopping behavior. The implications are worth pausing on: decades of fraud detection machine learning, built on patterns like mouse movement variability, browsing time, session behavior, device fingerprinting — all of it useless when the buyer is software. Agent traffic doesn’t move a mouse. It doesn’t browse. It doesn’t exhibit the behavioral variability that distinguishes a legitimate human shopper from a bot. Stripe had to build an entirely new fraud model for a client that is, by any prior definition, a bot. Brands including URBN, Etsy, Coach, Kate Spade, and Revolve are already onboarding.  Google launched its Agent Payments Protocol in September. PayPal and OpenAI partnered on instant checkout in ChatGPT. Visa built a Trusted Agent Protocol. At NRF 2026 in January, Google announced the Universal Commerce Protocol — an open standard for agent-to-commerce interaction — and Stripe’s ACS auto-supports it, meaning merchants who integrated Stripe’s agent tools are already compatible with Google’s agent shopping infrastructure without writing another line of code. The industry consensus, as a Decrypt analysis put it: “agents that can’t spend money are fundamentally limited.” Every major payment company reached this conclusion independently, within the same six-month window.  Now consider content access. The web is made of HTML, and HTML is designed for human browsers, not language models. Pages are bloated with scripts, tracking pixels, navigation menus, and ads. When an agent needs to read a webpage, it has to strip all of that away and convert to something useful — usually markdown. This is such a common step that an entire category of companies (Firecrawl, Jina, Exa) exists just to do the conversion.  Cloudflare’s “Markdown for Agents” cuts out the middleman. When an AI agent requests a page from any Cloudflare-enabled site, it sends an Accept: text/markdown header. Cloudflare intercepts the request, fetches the HTML from the origin server, converts it to markdown on the fly, and serves it back. The response even includes an x-markdown-tokens header with the estimated token count so the agent can manage its context window. No scraping, no conversion libraries, no wasted compute. The agent asks for markdown and gets markdown.  This matters more than it sounds. Cloudflare serves roughly 20% of all web traffic. When they decide agents are first-class citizens of the web — not visitors to be blocked, but clients to be served in their preferred format — that’s an infrastructure-level commitment to a world where software reads websites as routinely as humans do.  And Cloudflare isn’t stopping at markdown conversion. They launched three companion features in the same release. First, llms.txt and llms-full.txt — standardized, machine-readable site maps that tell agents what’s on a site and how to navigate it, the way robots.txt told search engine crawlers two decades ago. Second, AI Index — an opt-in search index where sites can make their content discoverable to agents directly through Cloudflare’s MCP server and search API, bypassing Google entirely. Third, and most telling: built-in x402 monetization support, so site owners can charge agents for content access using the same protocol that Coinbase’s wallets speak. Cloudflare isn’t just making the web readable for agents. They’re building the economic layer for a web where agents pay to read.  Then there’s search. Google Search is optimized for humans: ten blue links, ads, featured snippets, knowledge panels. None of that is useful to an agent that needs to programmatically find specific information and return structured data. Exa.ai built a search engine from scratch specifically for agents — their own index, their own neural retrieval models, their own embedding infrastructure. Their API returns raw URLs and content, not SERP pages. Their research endpoint chains multiple searches together agentically, parallelizing across output fields to minimize latency. It scores 94.9% on SimpleQA, a benchmark for factual accuracy. For comparison, GPT-4.1 and Perplexity score lower.  The benchmark results are less interesting than what they imply about market structure. Google built a search engine for humans and spent two decades perfecting it. Now there’s a parallel need — search for machines — and Google’s architecture is the wrong shape for it. The companies that build agent-native search from first principles have an actual structural advantage, not just a marketing one.  An independent benchmark from AiMultiple tested the major agent search providers head-to-head. Brave Search led on composite agent score. Firecrawl, Exa, and Parallel Pro were statistically tied behind it. But the latency spread tells you where the real differentiation lives: Brave returned results in 669 milliseconds; Parallel Pro took 13.6 seconds. In an agent workflow where each search is one step in a chain of ten, that latency difference compounds into minutes. The providers that own their own infrastructure and index — rather than wrapping Google’s API — have a structural speed advantage that grows more valuable as agent workflows get more complex.  And then there’s execution. OpenAI’s blog post on Skills, Shell, and Compaction reads like a roadmap for turning agents from advisors into workers. Skills are reusable, versioned instruction bundles — think “standard operating procedures for AI” — that an agent can load on demand. The Shell tool gives agents a real terminal environment where they can install dependencies, run scripts, and write output files. Compaction manages the context window automatically so long-running agent workflows don’t crash when they hit token limits.  The details matter here because they reveal OpenAI’s bet about what agent architecture actually looks like in production. Skills aren’t prompts. They’re versioned, mountable instruction packages — more like Docker images than chat templates. An organization can build a Salesforce skill, test it, lock the version, and deploy it across every agent in the company with the guarantee that every agent follows the same procedure. When the procedure changes, you update the skill version, not every agent’s system prompt. That’s the difference between artisanal prompt engineering and actual software engineering applied to AI operations.  The Shell tool is equally telling. It gives agents a real Linux environment — not a sandboxed playground, but a terminal where they can pip install, curl, grep, and write files to disk. The pattern OpenAI describes — install dependencies, fetch external data, produce a concrete deliverable — is functionally identical to how a human freelancer works. Read the brief. Set up the tools. Do the research. Deliver the artifact. The difference is that the agent can do it inside a container in seconds, and Skills ensure it follows the same procedure every time.  Glean, an enterprise search company that was an early Skills customer, saw accuracy on Salesforce-related tasks jump from 73% to 85% with a single well-structured skill. Time-to-first-token dropped 18.1%. The gains came from moving stable procedures out of the system prompt and into versioned, modular instruction bundles — which is just software engineering applied to AI workflows. Nothing revolutionary about the concept. Everything revolutionary about the implication: enterprise AI deployments can now be managed like software deployments, with version control, testing, rollback, and standardized procedures across an entire organization.  And then there’s Compaction — the least flashy feature and arguably the most important. Long-running agent workflows accumulate context: pages of search results, API responses, intermediate calculations, conversation history. Eventually, the context window fills up and the agent either crashes or starts forgetting early steps. Compaction handles this server-side, automatically summarizing and compressing context to keep the agent operational across workflows that would otherwise be impossible. It’s the feature that makes agents viable for tasks that take hours instead of seconds — the kind of sustained, multi-step work that defines most real jobs. The Emergent Web What happens when you combine all of these primitives? An agent with a wallet, search capabilities, content access, payment rails, and an execution environment is not an assistant. It’s an economic actor.  Consider what a developer calling himself @chatcutapp demonstrated on X this week. He connected OpenClaw to Seedance 2.0 (a video generation model) inside an app called ChatCut. Then he sent the agent an Amazon product link. The agent crawled the Amazon page, extracted product information and photos, identified which assets were suitable for video generation, fed them into Seedance, and produced a UGC-style product video — the kind of content that brands pay creators $500–$2,000 to produce. No human touched any step between “paste this link” and “here’s your video.”  That’s the emergent web. Not a single agent doing a single task, but agents chaining capabilities together across services to produce outputs that previously required multiple humans and multiple tools. The Amazon page wasn’t designed for agents. Seedance 2.0 wasn’t designed to receive input from web crawlers. ChatCut wasn’t designed as an orchestration layer. But because each piece exposes its capability through APIs and structured data, the agent can stitch them together into a workflow that no single company planned.  This is the pattern that the infrastructure convergence makes inevitable. When content is available as markdown, search returns structured data, execution happens in containers, and payment flows through tokenized protocols, the agent doesn’t need anyone to build an integration between Service A and Service B. It reads both, understands both, and chains them together on the fly. The emergent web isn’t a platform anyone builds. It’s what happens automatically when the primitives exist and the agent is smart enough to combine them.  The implications for the creator economy alone are staggering. That UGC product video would have cost a brand $500–$2,000 from a human creator — find the product, research the angles, shoot the footage, edit, deliver. The agent replicated the workflow from a single link. The output won’t match a human creator’s judgment — but the cost approaches zero and the turnaround is measured in minutes. Multiply that by every content type that follows a repeatable pattern — product descriptions, social media posts, email campaigns, comparison articles — and you start to see why the infrastructure companies are building for scale that doesn’t exist yet. They’re building for the world where this is the default, not the demo.  Polymarket provides the most provocative case study of where this goes. The prediction market platform processed $12 billion in volume in January 2026 alone. Researchers from IMDEA Networks Institute analyzed 86 million bets and found that algorithmic traders extracted roughly $40 million in arbitrage profits over a twelve-month period. The top three wallets placed over 10,200 bets combined. Only 0.51% of all Polymarket users earned more than $1,000 — the rest were effectively providing liquidity for bots to extract value from.  And here’s where it gets interesting: Polymarket itself tweeted in early February that “autonomous AI agents are now trading on Polymarket in an attempt to subsidize their token costs.” Agents are trying to earn money to pay for their own compute. The loop is closing.  The data on how well they’re doing is mixed but illuminating. Olas Protocol’s Polystrat agents — among the most sophisticated autonomous prediction market systems publicly tracked — achieve 55–65% win rates over time, with performance varying dramatically by domain: 59–64% accuracy on business and science questions, 38–49% on arts and fashion, around 51% on sports. The agents are better at predicting things that follow from data than things that follow from culture or human behavior. That pattern is actually useful — it tells you exactly what kind of economic activity agents are well-suited for and what kind they’re not.  The cumulative volume on Polymarket hit $15.7 billion. Thirteen thousand new AI agents registered on Ethereum in a single day following the Coinbase wallet launch. The infrastructure is being built not because agents are reliably profitable, but because the handful that are profitable are profitable enough to fund an entire ecosystem of experimentation around them. The Scam and the Signal This is also where the scam lives. TikTok is flooded with videos of people claiming to turn $50 into $3,000 in 48 hours with AI trading bots on Polymarket. A creator named RichKuo posted a video four days ago — 6,662 likes, 2,320 bookmarks — with the caption “Can OpenClaw make good trades?” and a thumbnail reading “OPENCLAW TRADING BOT.” The engagement farming industrial complex has latched onto this narrative the way it latches onto every narrative that combines “AI” with “make money.”  The reality is considerably less glamorous. The bot that famously turned $313 into $438,000 in a month was running latency arbitrage — exploiting the millisecond gap between when Bitcoin moves on Binance and when Polymarket odds adjust. That requires co-located infrastructure with sub-10ms latency to Polygon’s RPC providers, custom signing implementations that bypass standard Python clients (too slow), and capital significantly larger than what any TikTok tutorial suggests. One developer who actually built and tested an autonomous Polymarket agent reported that Cloudflare blocks API requests from datacenter IPs, requiring custom bypass infrastructure just to place orders. Another found that running the bot for three days cost $200 in API fees alone.  The honest assessment: yes, sophisticated autonomous trading agents can generate returns on Polymarket. No, you cannot replicate this by following a TikTok tutorial and funding a $50 account. The infrastructure requirements, API costs, and competitive dynamics (execution times compressed from 30 seconds to under 800 milliseconds in six months) make this a game for well-capitalized technical operators, not retail experimenters.  But the underlying premise — that agents can participate in economic activity and generate revenue — is not a scam. Agents will transact autonomously — Coinbase, Stripe, Google, PayPal, Visa, and OpenAI are all building toward that with billions in infrastructure. The open issue is whether the guardrails get built fast enough to prevent the predictable disasters.  What the Polymarket data actually reveals is a segmentation that applies far beyond prediction markets. Agents excel at tasks where the inputs are structured, the logic is data-driven, and the feedback loop is tight. They struggle where context is cultural, judgment is aesthetic, or the variables are human. The 64% accuracy on business questions versus 39% on fashion questions isn’t just a Polymarket stat — it’s a map of where autonomous agent economics will work first and where they’ll fail longest. Finance, logistics, data analysis, code generation: high-accuracy domains where agents will generate real economic value. Creative direction, relationship management, cultural strategy: low-accuracy domains where the 70/30 human-agent split I discussed in the last piece will persist much longer than the infrastructure builders are planning for. The Security Problem I covered OpenClaw’s security nightmare in detail in the first piece — the CVE that allowed one-click remote code execution, the 341 malicious skills disguised as crypto tools on ClawHub, Cisco’s research team finding data exfiltration in a third-party skill, VirusTotal documenting malware distribution through the ecosystem. I’m not going to rehash all of that here. What I want to focus on instead is the structural problem that those incidents illustrate, because it scales with the infrastructure.  Every primitive that makes agents more capable also makes them more dangerous. An agent with a wallet can pay for APIs — or get drained by a malicious skill. An agent with shell access can install dependencies — or execute arbitrary code injected through a prompt. An agent with search can find information — or be redirected to adversarial content designed to manipulate its behavior. An agent with Cloudflare-served markdown can read websites efficiently — or consume poisoned content at machine speed with no human in the loop to notice.  The security community is already responding, and the responses are instructive because they reveal what serious people think the actual attack surface looks like. IronClaw, a Rust-based reimplementation of OpenClaw by Near.AI co-founder Illia Polosukhin, sandboxes every tool in isolated WebAssembly environments — the assumption being that any tool an agent touches is a potential compromise vector. OpenAI’s Shell tool includes org-level and request-level network allowlists, domain secrets that prevent credential leakage, and container isolation — the assumption being that agents will run untrusted code and the environment must contain the blast radius. Coinbase’s Agentic Wallets use enclave isolation for private keys and programmable spending guardrails — the assumption being that the agent itself cannot be fully trusted with the assets it manages.  Notice the pattern: every serious security approach treats the agent as a potential adversary, not a trusted employee. That’s the correct mental model for where we are right now, and it’s one that most of the TikTok-tutorial crowd has not internalized. What’s Actually New The individual components are not new. Agents have existed for years. APIs have existed for decades. The concept of software transacting with other software predates the web itself.  What’s new is the convergence. In the span of roughly six months — from Stripe’s Agentic Commerce Protocol in September 2025 to Coinbase’s Agentic Wallets and Cloudflare’s Markdown for Agents this week — every layer of the stack went from concept to production infrastructure. Money, content, search, execution, identity. All of it, simultaneously.  The web is forking. There’s the human web — the one you’re reading right now, with its fonts and layouts and images and scroll animations. And there’s the agent web — a parallel layer of APIs, structured data, markdown content, payment protocols, and execution environments designed for software that never opens a browser.  These two webs run on the same physical infrastructure — same servers, same CDNs, same payment rails — but they serve fundamentally different clients with fundamentally different needs. A human wants a beautiful product page; an agent wants a JSON payload with price, availability, and a payment endpoint. Where you and I expect search results we can browse, an agent needs structured data it can act on immediately. The checkout flow you trust — upsells, security badges, the whole reassurance architecture — is noise to software that just needs a tokenized payment primitive with programmable spending limits.  The analogy that keeps coming to mind is the early mobile web. In 2007, when the iPhone launched, the web already existed. It worked on phones — technically. But it was designed for desktops, and the experience was terrible. What followed was a decade-long rebuild: responsive design, mobile-first frameworks, app stores, push notifications, GPS-aware services, tap-to-pay. The underlying infrastructure was the same. The interface layer forked completely. The companies that recognized the fork early — that built for the new client instead of trying to make the old interface work on the new device — built the dominant platforms of the next era.  We’re at the same inflection point, except the new client isn’t a smaller screen. It’s not a screen at all. It’s software that reads, decides, pays, and acts. The interface it needs isn’t visual. It’s structured, programmable, and transactional. And the companies building that interface right now aren’t startups hoping to get lucky. They’re Coinbase, Stripe, Cloudflare, Google, OpenAI, Visa, and PayPal — companies with the infrastructure reach to make their design decisions into de facto standards.  The mobile fork created trillion-dollar companies — Uber, Instagram, WhatsApp, Snap — that couldn’t have existed on the desktop web. Not because the desktop web lacked capability, but because it lacked the interface primitives that mobile clients needed: real-time location, always-on connectivity, camera-first interaction, push notifications, tap-to-pay at physical registers. The agent fork will do the same thing. The businesses that emerge from it will be ones that couldn’t have existed on the human web — not because the human web lacks information, but because it lacks the interface primitives that agent clients need: structured data, tokenized payments, machine-readable content, programmatic search, and execution environments.  In the last piece, I talked about the 70/30 rule — the finding that people consistently want to maintain 70% human control over agent-delegated tasks. That’s the demand side. This piece is about the supply side, and the supply side doesn’t care about the 70/30 split. The infrastructure being built this month is designed for a 0/100 world — fully autonomous agents with their own wallets, their own search capabilities, their own execution environments, and their own economic relationships with the services they use. The gap between the infrastructure being built and the trust people are willing to extend is the central tension of the next few years. Every company in the agent stack is betting that trust will catch up to capability. Every security incident — every ClawHavoc, every 500-message iMessage disaster, every production database wiped by an unsupervised agent — pushes that timeline back.  For now, the agent web is small. It’s developers running OpenClaw on Mac Minis and VPS instances, AI shopping assistants placing orders through Stripe’s ACP, trading bots scanning Polymarket, and video generation agents crawling Amazon listings. But the infrastructure being built this month — by the largest technology companies on Earth, with billions of dollars in committed capital — is not designed for current scale. It’s designed for a world where agents are as common on the internet as browsers.  Whether that world arrives in two years or ten is an open question. That it’s being built right now is not.  I make this Substack thanks to readers like you! Learn about all my Substack tiers here and grab my prompt tool here     --- Invite your friends and earn rewards If you enjoy Nate’s Substack, share it with your friends and earn rewards when they subscribe.        ---

## Minhas Anotações

