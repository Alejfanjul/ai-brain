# Why "I'll get to AI eventually" is now the most expensive career decision you can make + the kit to start

## Fonte
- **Tipo:** newsletter
- **Autor:** Nate
- **URL:** email
- **Data original:** 2026-02-09
- **Data captura:** 2026-02-09

## Conteúdo

Why "I'll get to AI eventually" is now the most expensive career decision you can make + the kit to start

Watch now | Yes, AI is collapsing futures—but probably not in the way you think.

 --- Look at you getting killer career perspective and the full AI picture. Give yourself a pat on the back for diving in on AI and go get a coffee ☕   ---riverside_2.9_- ss video_nate_jones's studio.mp4 ---Watch now    Why "I'll get to AI eventually" is now the most expensive career decision you can make + the kit to startYes, AI is collapsing futures—but probably not in the way you think.  ---[Nate](https://substack.com/@natesnewsletter)  --- Feb 9  --- ∙ Paid  --- Not collapsing as in destroying. Collapsing as in compressing—across two dimensions simultaneously.  The first collapse is horizontal. Engineer, product manager, marketer, analyst, designer, ops lead—these used to be distinct career paths with distinct skill sets. They’re converging—fast—into a single meta-competency: orchestrating AI agents to get work done.  The second collapse is temporal. The leverage you thought you’d build over the next five years? That timeline is compressing into the next five months. The rate of AI capability improvement nearly doubled in April 2024, according to Epoch AI’s analysis of frontier models. We’re not on the same curve we were on two years ago. We’re on a steeper one.  Both collapses point to the same conclusion: now is what matters. Not eventually. Not when things settle. The preparation is the engagement.  **Here’s what’s inside:**   * **The horizontal collapse.** Why fifty career specializations are converging into one meta-skill—and what that skill actually is. * **The temporal collapse.** The data showing AI capability improvement nearly doubled in 2024, and why “I’ll get to it eventually” is now the most expensive career decision you can make. * **The capex confirmation.** Over $630 billion in 2026 infrastructure spending tells you everything about what the biggest companies in history believe is coming. * **The objections that expired.** Hallucinations, maturity, job immunity, time—each one addressed with current data. * **The bike metaphor.** Why going faster is actually safer than going slower, and what that means for how you engage right now.   Let me walk you through both collapses—and then show you what to do about them.  Subscribers get all posts like these!     **Grab the Prompts** Reading about disruption doesn’t change how you work. Doing something different tomorrow morning does. These prompts force three decisions most people defer indefinitely: where you actually stand today (not where you think you stand), what “directing AI” looks like in your specific domain (not in the abstract), and what you’re going to do this week (not “eventually”). The Collapse Position Audit in particular exists because I’ve watched too many smart people skip the honest self-assessment and jump straight to tool shopping — and then wonder six months later why nothing stuck. If you can’t fill in the blanks, you’re not ready to build a plan. **The Horizontal Collapse: One Skill to Rule Them All** Most people haven’t fully processed this yet: the differentiation between knowledge work roles is eroding.  Gartner predicts 40% of enterprise applications will integrate task-specific AI agents by the end of 2026—up from less than 5% in 2025. An eightfold increase in eighteen months.  That number lands differently when you look at where organizations already are. G2’s August 2025 enterprise survey found 57% of companies already have AI agents in production. PwC’s May 2025 survey of 300 senior executives puts it even higher—79% say agents are already being adopted. McKinsey’s research on the “agentic organization” describes what comes next: org charts that pivot from hierarchical delegation to “agentic networks,” where humans and AI agents exchange tasks and outcomes fluidly.  What does this mean for your career? It means the specific domain expertise that made you valuable is becoming table stakes. The differentiator is no longer whether you understand marketing or engineering or finance. The differentiator is whether you can effectively direct AI systems to execute marketing, engineering, or finance tasks at scale.  Think about what a product manager does today versus two years ago. The job used to require synthesizing customer feedback, writing specs, coordinating with engineering, managing stakeholders. Now? The job increasingly involves prompting models to draft specs, using AI to analyze customer data, deploying agents to track project status. The output is similar. The how is radically different. And the people who master the new how are producing three to five times more output than those who don’t. I want to be careful with that number—it’s what I’ve observed in teams I work with, not a controlled study. But the gap between people who’ve internalized this shift and people who haven’t is large enough that you notice it in weeks, not months.  This pattern repeats across every function. Legal teams are using AI to review contracts in hours instead of weeks. Marketing is deploying agents to personalize campaigns across thousands of customer segments simultaneously. Engineering teams have AI writing first-draft code, reviewing pull requests, and generating documentation—while finance uses models to build projections that used to take analysts days. The common thread isn’t the domain. It’s the shape of the work: a human directing an AI system toward an outcome that used to require a specialized team.  The World Economic Forum’s Future of Jobs Report 2025 found that employers expect 39% of workers’ core skills to change by 2030—with AI management rising fastest among all technological skills. KPMG’s AI Quarterly Pulse Survey (Q4 2025) found 76% of leaders would pay up to 10% more for candidates with strong AI skills. New roles are emerging—AI trainer, AI performance analyst, agent orchestrator—but they’re all variations on the same underlying competency.  What used to be fifty different specializations is converging into variations on a single theme: human directing AI directing outcome. The horizontal paths are folding into one vertical.  Something crucial gets lost in the doom-and-gloom framing: this is an art you learn by doing. You don’t learn to ride a horse by reading a book. You don’t learn to swim by sitting in a deck chair watching the ocean. AI is experiential technology. The patterns only click when you’re in there trying things, breaking things, iterating.  And there’s a specific mental model that separates people who struggle with AI from people who thrive: software-shaped intent. When you direct an agent to do something, you need to think in terms of what agents can actually deliver within the technical ecosystem they occupy. Where is the agent’s tool set? Where is its memory? Where does it read data, and where does it write? Software is leverage expressed in silicon—and so much of it is just reading, writing, and presenting data in useful ways.  This used to be an engineering-only way of thinking. Product people had to learn it. Now everyone does. When you start thinking in software-shaped terms, you can apply whatever domain knowledge you have—design, finance, customer success, legal—and direct AI agents far more effectively, even if your job has never involved building software.  The implications are profound and uncomfortable. That law degree you spent three years earning is still valuable—but the premium it commands is shrinking relative to your ability to deploy AI for legal research and document review. Same with the MBA, the decade of engineering experience, the hard-won domain expertise in finance or operations. The knowledge still matters. What’s changed is that it’s no longer sufficient on its own—the differentiator has shifted to whether you can direct AI systems to apply that knowledge at scale.  The expertise doesn’t disappear. But it becomes foundational rather than differentiating. Everyone needs domain knowledge to direct AI effectively in that domain. But the direction itself—the meta-skill of agent orchestration—is what separates high performers from everyone else. **The Temporal Collapse: Your Future Is Now** The second collapse is even more disorienting. Career advantage—the accumulated edge you build over time—is compressing into the present moment.  Consider the data. On the SWE-bench coding benchmark, AI systems could solve just 4.4% of problems in 2023. By 2024, that number hit 71.7%. In one year. The Epoch AI Capabilities Index shows frontier model improvement accelerating from roughly 8 points per year before April 2024 to over 15 points per year after—an 85% acceleration that coincided with the rise of reasoning models and increased focus on reinforcement learning. METR’s research found that the length of tasks AI can reliably complete has been doubling roughly every seven months since 2019, with suggestive evidence of acceleration to approximately every four months in 2024.  The doubling time itself may be shrinking.  Traditional career planning assumed you had time. Learn a skill, apply it for years, build expertise, get promoted, eventually leverage that expertise into leadership or specialization. The timeline gave you breathing room. You could wait for a technology to mature before engaging with it. You could let the early adopters work out the kinks. You could be strategic about when to invest your learning energy.  That assumption is now catastrophically wrong. I’m not entirely sure what the right response is to a curve this steep—I don’t think anyone is, if they’re being honest. But I know what the wrong response is: pretending the curve isn’t there.  The inference cost for GPT-3.5-level performance dropped 280-fold between November 2022 and October 2024. Eighteen months. What cost $20 per million tokens now costs $0.07. What required 540 billion parameters in 2022 (Google’s PaLM) achieves equivalent scores with 3.8 billion parameters today—a 142-fold reduction in model size. The Stanford AI Index reports that depending on the task, LLM inference prices have fallen anywhere from 9 to 900 times per year.  The technology isn’t just improving. The improvement itself is accelerating. And the cost of accessing that improvement is collapsing even faster.  This creates a brutal dynamic for career planning. The skills that will matter in 2027 are being defined now, by the people engaging now. If you wait until the technology “settles down,” you’ll find that the early adopters have already built the workflows, established the norms, and captured the opportunities you were waiting for. They’ll have two years of compound learning while you’re still figuring out the basics.  The technology isn’t going to settle down into a stable state you can evaluate from a distance. The curve is steepening, and it rewards the people who climb it early.  Here’s where I want to flip the script on how this usually gets framed. People hear “accelerating change” and think: terrifying. But there’s a better metaphor.  Think about learning to ride a bike. When you’re going slow, it’s incredibly hard to balance. You feel wobbly, unstable, like you’re constantly about to fall. Kids struggle with this—they think if they go slower, they’ll be safer. But the opposite is true. When you speed up on a bike, the steadiness increases. Balance becomes easier. Going faster is actually safer than going slower.  AI works the same way. If you’re trying to go slow—dipping a toe in, waiting to see how things develop, trying to fit AI into your existing workflow without changing much—you’re going to feel constantly off-balance. Every new development will feel like a threat. Every capability jump will feel destabilizing. But if you lean in and go faster, something different happens. The patterns start to click. You develop intuition. The chaos starts to feel manageable because you’re moving with it rather than against it.  The old career model assumed your expertise appreciated over time like a traditional asset. You learned something valuable, and it stayed valuable, gradually compounding as you applied it to harder problems. The new model is different. Your expertise depreciates unless you continuously update it—and the depreciation rate is accelerating. What you learned about AI capabilities six months ago is already partially obsolete. What you learn today will be partially obsolete by summer.  This isn’t an argument for panic. It’s an argument for continuous engagement. The people who will thrive aren’t the ones who master AI once and coast. They’re the ones who develop the meta-skill of continuously learning and adapting as the technology evolves. The half-life of specific AI knowledge is short and getting shorter. The half-life of the learning habit is long and getting more durable. **The Capex Confirms It** If you doubt the magnitude of what’s happening, follow the money.  Big Tech’s combined AI capital expenditure hit $405 billion in 2025. Initial projections pegged 2026 at $500 billion; the latest spending plans from major tech firms now imply north of $630 billion. The hyperscalers—Amazon, Microsoft, Google, Meta, Oracle—are projected to deploy trillions in cumulative AI capex through 2030, according to Morgan Stanley’s analysis of their funding capacity. That’s not venture capital speculation. That’s operational investment by companies with direct visibility into enterprise demand.  Amazon’s AWS, Microsoft Azure, and Google Cloud collectively account for 66% of global cloud infrastructure spending, which reached $102.6 billion in Q3 2025 alone—a 25% year-over-year increase. These companies aren’t building infrastructure hoping someone will use it. They’re racing to fulfill demand they can already see.  Bank of America notes that AI capex now consumes up to 94% of these companies’ operating cash flows after dividends and buybacks—meaning nearly every discretionary dollar is going into AI infrastructure. When the largest technology companies in history dedicate nearly all of their available capital to a single thesis, you’re not watching a trend. You’re watching a phase transition.  The money is committed. The infrastructure is being built. The enterprise contracts are signed. The hyperscalers have made their bet. They believe AI will define the next era of computing so thoroughly that they’re willing to spend everything to capture it. The question is whether you’re willing to act with the same conviction about your own career. **The Objections That No Longer Apply** This is where things get uncomfortable.  Many people are still resisting. They don’t have time. They tried ChatGPT in 2022 and it hallucinated. They read an article about AI limitations. They heard from a friend that the hype is overblown. They have a hundred reasons why now isn’t the right time to engage seriously.  I’ve heard all of them. And I understand the psychology. Change is hard. Admitting that skills you’ve spent years developing might be diminishing in value is genuinely painful. The resistance isn’t irrational—it’s protective. Your brain is trying to defend your sense of competence and identity from a threatening new reality.  But protection has costs, and they compound faster than most people realize.  The hallucination critique was valid. In 2022. Models did make things up, frequently and confidently. If you tested ChatGPT early and walked away unimpressed, that was a reasonable reaction to what you experienced. The problem is that you’re not evaluating 2022 models anymore. You’re living in a world where Claude and GPT-4 and Gemini, with proper prompting and tool use, produce outputs that compare favorably to human knowledge workers on many tasks. Not perfect. But good enough to be useful at scale. And “good enough to be useful” is the threshold that matters, because useful tools get adopted, and adopted tools improve, and improving tools widen the gap.  The “I’ll wait until it matures” objection was reasonable when technology adoption followed predictable fifteen-year curves. The internet took roughly a decade to go from curiosity to infrastructure. Mobile took nearly as long. You could afford to be a fast follower rather than an early adopter. But those timelines assumed gradual improvement. AI capability improvement nearly doubled in 2024. The pace is accelerating, not stabilizing. By the time you’ve waited for maturity, the landscape has transformed, and the people who engaged early have captured the opportunities you were waiting for.  The “my job is immune” objection is the most dangerous, because it feels so plausible. Of course your specific combination of skills and relationships and domain knowledge can’t be replicated by AI. Of course there’s something special about what you do that machines can’t touch.  Maybe. But probably not as much as you think.  Every knowledge worker who processes information and makes decisions is working in a domain that AI is penetrating. Legal research, financial modeling, marketing analysis, code review, customer communication, strategic planning—all of it is being augmented, accelerated, and in some cases automated. The data is consistent across sources: PwC’s May 2025 survey of 300 senior executives found 79% say agents are already being adopted in their companies. G2’s August 2025 enterprise survey puts 57% of companies with AI agents already in production. In joint MIT Sloan Management Review and BCG research on agentic AI organizations, 45% of “agentic AI leaders” expect fewer middle-management layers. The functions that remain fully human are shrinking, not growing.  The “I don’t have time” objection is perhaps the most ironic. You don’t have time to learn tools that could multiply your productivity by 3-5x? You don’t have time to develop skills that every survey says employers are prioritizing? You don’t have time to engage with the technology that’s consuming over $400 billion in annual infrastructure investment?  The people who “don’t have time” for AI are going to find themselves with plenty of time—just not the kind they wanted. **Respecting the Exit** Now, an important distinction.  I have genuine respect for people who look at this landscape clearly and choose to leave.  I know founders who have stepped back from tech entirely. They’re taking carpentry classes. Opening bookshops. Building physical things with their hands. They looked at the trajectory of AI-augmented knowledge work and decided, consciously and deliberately, that it’s not the life they want.  That’s a legitimate choice. It’s honest. It’s self-aware. They’re not pretending that their resistance is temporary or strategic. They’re not telling themselves they’ll engage eventually, once things settle. They’ve made a clear-eyed assessment and chosen a different path entirely.  The exit is honorable. What isn’t is the middle position—staying in knowledge work while pretending the landscape isn’t shifting beneath you. The people who stay in knowledge work, who continue climbing what they imagine to be traditional career ladders, while refusing to engage with the single largest shift their industry has ever seen—that’s not strategy. It’s denial dressed up as patience, and it tends to make everyone around you miserable. Including you.  If you’re going to stay in fields touched by AI—which is increasingly everything involving a computer—you have to engage. If you’re not going to engage, you should seriously consider whether staying is the right choice.  And I’ll be the first to admit that the bike metaphor I’m about to use has limits—riding a bike doesn’t fundamentally change what a bike is, and AI is changing the nature of the work itself. But the balance part holds. **The Invitation** I want to be honest about something: none of us chose this.  I didn’t choose for timelines to compress. You didn’t choose for career paths to converge. The industry as a whole made that choice, and we’re all living through this moment together. The ground shifted beneath us while we were standing on it.  It’s easy to look at everything I’ve laid out and feel doom and gloom. The accelerating curves, the collapsing differentiation, the relentless pace of change—it can feel overwhelming. I get it.  But here’s what I’ve seen, over and over, across dozens of people in wildly different fields—healthcare, tech, finance, engineering, product, even small-town community building: when people recognize what’s happening and choose to engage with curiosity rather than resistance, everything changes.  Curiosity literally opens up your brain. We need that openness to navigate this AI world in a way that works for us rather than against us. And without exception, the people I’ve watched choose that positive path—choose to lean in even when they weren’t sure, choose to try the next thing and then the next thing—have gone farther than they expected.  So if I can leave you with anything in the middle of a timeline that feels increasingly wild and unpredictable, it’s this: get on the bike.  Try Claude Code. Try Lovable. Try a different way of working with whatever chatbot you use. And then do the next thing. Lean in a little farther. Go a little faster. The patterns will start to solidify. Your unconscious brain will pick up how AI works across systems. And over time, it’s going to feel steadier—not because the world slowed down, but because you sped up.  Going faster is safer than going slower—and, counterintuitively, less scary too. That’s not a platitude. It’s what I’ve watched happen, person after person, once they stop white-knuckling the handlebars and actually pedal.  The funnel is real. The collapse is happening. But on the other side of engaging with it is a kind of stability that you can’t get any other way.  Now is what matters. And now is a good time to start.  I’ll leave you with some wise words from a legend:   --- I make this Substack thanks to readers like you! Learn about all my Substack tiers here and grab my prompt tool here     --- Invite your friends and earn rewards If you enjoy Nate’s Substack, share it with your friends and earn rewards when they subscribe.        ---

## Minhas Anotações

